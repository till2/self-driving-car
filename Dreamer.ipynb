{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b1c5dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/till/.local/lib/python3.10/site-packages/gymnasium/envs/registration.py:498: UserWarning: \u001b[33mWARN: Overriding environment GymV26Environment-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import gym_donkeycar\n",
    "\n",
    "import os\n",
    "from ruamel.yaml import YAML\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import HBox, VBox\n",
    "from IPython.display import display\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import distributions as dist\n",
    "from torch.distributions import Normal, Categorical\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from tensorboard import notebook\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Set the environment variable to suppress TensorFlow warning\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# custom classes and functions\n",
    "import models\n",
    "from models.blocks import ConvBlock, TransposeConvBlock, ResConvBlock, CategoricalStraightThrough\n",
    "from models.mlp import MLP\n",
    "from models.categorical_vae import CategoricalVAE\n",
    "from models.sequential_categorical_vae import SeqCatVAE\n",
    "from preprocessing import grayscale_transform as transform\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7cd6cd",
   "metadata": {},
   "source": [
    "## Load Hyperparameters from YAML config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "501f6016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'debug': False,\n",
       " 'logdir': 'logs/',\n",
       " 'seed': 0,\n",
       " 'precision': 32,\n",
       " 'device': 'cuda:0',\n",
       " 'size': [128, 128],\n",
       " 'grayscale': True,\n",
       " 'toy_env': True,\n",
       " 'n_episodes': 5000,\n",
       " 'max_episode_steps': 100,\n",
       " 'env_id': 'donkey-generated-roads-v0',\n",
       " 'max_grad_norm': 100,\n",
       " 'batch_size': 8,\n",
       " 'H': 512,\n",
       " 'num_categoricals': 32,\n",
       " 'num_classes': 32}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yaml = YAML(typ='safe')\n",
    "with open(\"./config.yaml\", \"r\") as file:\n",
    "    config = yaml.load(file)\n",
    "\n",
    "for key in config:\n",
    "    locals()[key] = config[key]\n",
    "\n",
    "# set dependent parameters\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() and config[\"device\"] == \"cuda:0\" else \"cpu\")\n",
    "A = 2 if config[\"toy_env\"] else 3\n",
    "Z = config[\"num_categoricals\"] * config[\"num_classes\"]\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55c4883",
   "metadata": {},
   "source": [
    "## Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c36db89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_np = lambda x: x.detach().cpu().numpy() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21737dc6",
   "metadata": {},
   "source": [
    "## VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9220a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing encoder:\n",
      "- adding ConvBlock((1, 16))                   ==> output shape: (16, 64, 64) ==> prod: 65536\n",
      "- adding ConvBlock((16, 32))                   ==> output shape: (32, 32, 32) ==> prod: 32768\n",
      "- adding ConvBlock((32, 64))                   ==> output shape: (64, 16, 16) ==> prod: 16384\n",
      "- adding ConvBlock((64, 128))                   ==> output shape: (128, 8, 8) ==> prod: 8192\n",
      "- adding ConvBlock((128, 64))                   ==> output shape: (64, 4, 4) ==> prod: 1024\n",
      "- adding Flatten()\n",
      "- adding Reshape: (*,1024) => (*,32,32)\n",
      "\n",
      "Initializing decoder:\n",
      "- adding Reshape: (*,1024) => (*,64,4,4)\n",
      "- adding transpose ConvBlock(64, 64)                   ==> output shape: (64, 8, 8) ==> prod: 4096\n",
      "- adding transpose ConvBlock(64, 128)                   ==> output shape: (128, 16, 16) ==> prod: 32768\n",
      "- adding transpose ConvBlock(128, 64)                   ==> output shape: (64, 32, 32) ==> prod: 65536\n",
      "- adding transpose ConvBlock(64, 32)                   ==> output shape: (32, 64, 64) ==> prod: 131072\n",
      "- adding transpose ConvBlock(32, 1)                   ==> output shape: (1, 128, 128) ==> prod: 16384\n",
      "\n",
      "| CategoricalVAE info |\n",
      "------------------------\n",
      "device: cuda\n",
      "number of parameters: 1,121,200 (encoder: 418,448, decoder: 702,752)\n",
      "input shape : [8, 1, 128, 128]\n",
      "hidden shape: [8, 32, 32]\n",
      "viewing z as (*,64, 4, 4)\n",
      "output shape: [8, 1, 128, 128]\n",
      "entropyloss_coeff: 0.0\n",
      "uniform_ratio: 0.01\n"
     ]
    }
   ],
   "source": [
    "vae = CategoricalVAE().to(device)\n",
    "vae.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239558a5",
   "metadata": {},
   "source": [
    "## RSSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5aaf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSSM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RSSM, self).__init__()\n",
    "        \n",
    "        # init the VAE\n",
    "        self.vae = SeqCatVAE(H, Z, grayscale=True, vae_ent_coeff=1e-8)\n",
    "        \n",
    "        # init the RNN\n",
    "        self.num_rnn_layers = 1\n",
    "        self.rnn = nn.GRU(input_size=A+H+Z, hidden_size=H, num_layers=self.num_rnn_layers)\n",
    "        \n",
    "        # init MLPs\n",
    "        self.dynamics_mlp = MLP(input_dims=H, output_dims=Z) # H -> Z\n",
    "        self.reward_mlp = MLP(input_dims=H+Z, output_dims=1) # state (H+Z) -> 1\n",
    "        self.continue_mlp = MLP(input_dims=H+Z, output_dims=1) # state (H+Z)->1 # add sigmoid and BinaryCE  \n",
    "    \n",
    "    def step(self, action, h, z):\n",
    "\n",
    "        # concatenate the rnn_input and apply RNN to obtain the next hidden state\n",
    "        rnn_input = torch.cat((action, h.view(-1, H), z), 1)\n",
    "        _, h = self.rnn(rnn_input, h.view(-1, H))\n",
    "        \n",
    "        state = torch.cat((h.view(-1, H), z), 1)\n",
    "        \n",
    "        # predict the reward and continue flag\n",
    "        reward_pred = rssm.reward_mlp(state)\n",
    "        continue_prob = torch.sigmoid(rssm.continue_mlp(state)) # binary classification\n",
    "        continue_pred = bool(continue_prob > 0.5)\n",
    "        \n",
    "        x_reconstruction = rssm.vae.decode(h, z)\n",
    "        \n",
    "        \n",
    "        return h, reward_pred, continue_prob, continue_pred, x_reconstruction\n",
    "    \n",
    "    def get_losses(self,\n",
    "                   x_target, x_pred, \n",
    "                   reward_target, reward_pred, \n",
    "                   continue_target, continue_prob, \n",
    "                   z_pred, z):\n",
    "        \n",
    "        image_loss = F.mse_loss(x_target, x_pred, reduction=\"mean\")\n",
    "        reward_loss = F.mse_loss(reward_target.squeeze(), reward_pred.squeeze(), reduction=\"mean\")\n",
    "        continue_loss = F.binary_cross_entropy(continue_prob.squeeze(), continue_target.squeeze())\n",
    "        \n",
    "        # DreamerV3 KL losses: regularize the posterior (z) towards the prior (z_pred)\n",
    "        kld = dist.kl.kl_divergence\n",
    "        \n",
    "        # define the distributions with grad\n",
    "        dist_z = dist.OneHotCategorical(probs=z.view(-1, num_categoricals, num_classes))\n",
    "        dist_z_pred = dist.OneHotCategorical(probs=z_pred.view(-1, num_categoricals, num_classes))\n",
    "        \n",
    "        # define the distributions without grad\n",
    "        dist_z_sg = dist.OneHotCategorical(probs=z.detach().view(-1, num_categoricals, num_classes))\n",
    "        dist_z_pred_sg = dist.OneHotCategorical(probs=z_pred.detach().view(-1, num_categoricals, num_classes))\n",
    "\n",
    "        # calculate the mean KL-divergence across the categoricals\n",
    "        \n",
    "        dyn_loss = torch.max(torch.tensor(1), torch.mean(kld(dist_z_sg, dist_z_pred)))\n",
    "        rep_loss = torch.max(torch.tensor(1), torch.mean(kld(dist_z, dist_z_pred_sg)))\n",
    "     \n",
    "        # calculate the combined loss\n",
    "        loss = 1.0 * (image_loss + reward_loss + continue_loss) + 0.5 * dyn_loss + 0.1 * rep_loss\n",
    "        \n",
    "        return {\"loss\": loss, \"image_loss\": image_loss, \"reward_loss\": reward_loss, \n",
    "                \"continue_loss\": continue_loss, \"dyn_loss\": dyn_loss, \"rep_loss\": rep_loss}\n",
    "    \n",
    "    # info\n",
    "    # n_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda9f099",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rssm = RSSM().to(device)\n",
    "\n",
    "rssm_optim = optim.Adam(\n",
    "    rssm.parameters(), \n",
    "    lr=1e-4,\n",
    "\n",
    "    # l2 regularizer\n",
    "    weight_decay=1e-6, \n",
    ")\n",
    "\n",
    "value_net = MLP(input_dims=Z, output_dims=1).to(device) # state (H+Z) -> 1\n",
    "policy_net = MLP(input_dims=Z, output_dims=A).to(device) # state (H+Z) -> A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856cb39a",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd076d3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\" training loop \"\"\"\n",
    "\n",
    "toy_env = True\n",
    "n_episodes = 5000\n",
    "rssm.train()\n",
    "\n",
    "# Create the environment\n",
    "if toy_env:\n",
    "    assert A==3\n",
    "    env = gym.make(\"CarRacing-v2\", max_episode_steps=100, render_mode=\"rgb_array\") # rgb_array/human # 50 steps\n",
    "else:\n",
    "    assert A==2\n",
    "    exe_path = \"/home/till/Desktop/Thesis/donkeycar_sim/DonkeySimLinux/donkey_sim.x86_64\"\n",
    "    port = 9091\n",
    "    config = {\n",
    "        \"exe_path\" : exe_path, \n",
    "        \"port\" : port\n",
    "    }\n",
    "    env = gym.make(\n",
    "        \"GymV21Environment-v0\", \n",
    "        env_id=\"donkey-generated-roads-v0\", # donkey-minimonaco-track-v0 \n",
    "        max_episode_steps=100,\n",
    "        make_kwargs={\n",
    "            \"conf\": config\n",
    "        })\n",
    "\n",
    "# Logging\n",
    "log_dir = \"logs/\"\n",
    "writer = SummaryWriter(log_dir)\n",
    "notebook.start(f\"--logdir={log_dir}\")\n",
    "\n",
    "episode_losses = { # for loss plots\n",
    "    \"episode_loss\": [],\n",
    "    \"episode_image_loss\": [],\n",
    "    \"episode_reward_loss\": [],\n",
    "    \"episode_continue_loss\": [],\n",
    "    \"episode_dyn_loss\": [],\n",
    "    \"episode_rep_loss\": [],\n",
    "}\n",
    "\n",
    "try:\n",
    "    for episode in tqdm(range(n_episodes)):\n",
    "\n",
    "        # Get the initial state\n",
    "        obs, info = env.reset()\n",
    "\n",
    "        # Reset the RNN's hidden state\n",
    "        h = torch.zeros(rssm.num_rnn_layers, 1, H, device=device) # seq_len, B, H\n",
    "\n",
    "        # Add a new loss for the current episode and initialize it to 0\n",
    "        episode_length = 0\n",
    "        for key in episode_losses:\n",
    "            episode_losses[key].append(torch.tensor(0, device=device, dtype=torch.float32))\n",
    "\n",
    "        # Play one episode\n",
    "        done = False\n",
    "        while not done:\n",
    "\n",
    "            x = transform(obs).view(-1, 1, 128, 128)\n",
    "\n",
    "            \"\"\" WORLD MODEL LEARNING \"\"\"\n",
    "\n",
    "            # predict z and generate the true stochastic latent variable z with the encoder\n",
    "            z_pred = rssm.dynamics_mlp(h).view(-1, num_categoricals, num_classes) # (1,32,32) for the softmax\n",
    "            z_pred = F.softmax(z_pred, -1).view(-1, Z) # flatten to (1, 1024)\n",
    "            z = rssm.vae.encode(h, x).view(-1, Z)\n",
    "\n",
    "            # apply external actor and critic nets on z\n",
    "            action = policy_net(z) # Ax1 vector\n",
    "            v = value_net(z)\n",
    "\n",
    "            # predict one step using the RSSM and apply the actor-critic\n",
    "            h, reward_pred, continue_prob, continue_pred, x_pred = rssm.step(action, h, z)\n",
    "\n",
    "            # choose and execute an action\n",
    "            next_obs, reward, terminated, truncated, info = env.step(to_np(action.squeeze()))        \n",
    "\n",
    "            done = terminated or truncated\n",
    "            obs = next_obs\n",
    "\n",
    "            # calculate the loss\n",
    "            continue_target = torch.tensor(1 - done, device=device, dtype=torch.float32)\n",
    "            reward = torch.tensor(reward, device=device, dtype=torch.float32)\n",
    "\n",
    "            # TODO: z_prior, z_posterior\n",
    "            z_prior = torch.tensor(0, device=device, dtype=torch.float32)\n",
    "            z_posterior = torch.tensor(0, device=device, dtype=torch.float32)\n",
    "\n",
    "            losses = rssm.get_losses(x, x_pred, reward, reward_pred, \n",
    "                                     continue_target, continue_prob, z_pred, z)\n",
    "\n",
    "            # Add loss for the current step to the episode loss\n",
    "            episode_length += 1\n",
    "            for key in losses:\n",
    "                episode_losses[\"episode_\" + key][-1] += losses[key]\n",
    "\n",
    "        # Calculate the mean loss of the episode\n",
    "        for key in episode_losses:\n",
    "            episode_losses[key][-1] /= episode_length\n",
    "\n",
    "        # update the world model at the end of an episode using the mean loss of the episode\n",
    "        rssm_optim.zero_grad()\n",
    "        episode_losses[\"episode_loss\"][-1].backward()\n",
    "        nn.utils.clip_grad_norm_(rssm.vae.parameters(), max_norm=1000.0, norm_type=2)  \n",
    "        rssm_optim.step()\n",
    "\n",
    "        # Detach the losses to save memory and log them in TensorBoard\n",
    "        for key in episode_losses:\n",
    "            episode_losses[key][-1] = episode_losses[key][-1].detach().item()\n",
    "            writer.add_scalar(key, episode_losses[key][-1], global_step=episode)\n",
    "\n",
    "    env.close()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    \"\"\" Clean handling for interrupts to stop training early \"\"\"\n",
    "    print(\"Stopping training.\")\n",
    "    # Delete the last loss if the training was stopped early\n",
    "    # so that the list only consists of floats\n",
    "    for key in episode_losses:\n",
    "        if isinstance(episode_losses[key][-1], torch.Tensor):\n",
    "            episode_losses[key] = episode_losses[key][:-1]\n",
    "\n",
    "    # Close the TensorBoard writer and the gym environment\n",
    "    writer.close()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8335fef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU memory consumption:\n",
    "# 10 steps -> 6504MiB\n",
    "# 50 steps -> 19990MiB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e74d14",
   "metadata": {},
   "source": [
    "## Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0703ff02",
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_length = max(1, int(len(episode_losses[\"episode_loss\"])/20))\n",
    "\n",
    "fig, axs = plt.subplots(ncols=4, nrows=2, figsize=(4*5, 2*5))\n",
    "\n",
    "# Iterate over the keys and plot the losses\n",
    "for i, key in enumerate(episode_losses.keys()):\n",
    "    row = i // 4\n",
    "    col = i % 4\n",
    "\n",
    "    axs[row, col].set_title(key)\n",
    "    losses = episode_losses[key]\n",
    "    losses_moving_average = (\n",
    "        np.convolve(\n",
    "            np.array(losses).flatten(), np.ones(rolling_length), mode=\"valid\"\n",
    "        )\n",
    "        / rolling_length\n",
    "    )\n",
    "    axs[row, col].plot(range(len(losses)), losses, label=key)\n",
    "    axs[row, col].plot(range(len(losses_moving_average)), losses_moving_average, label=\"moving average\")\n",
    "    axs[row, col].legend(loc=\"upper right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3354e868",
   "metadata": {},
   "source": [
    "## Test area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46fa044",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ff0e83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30768921",
   "metadata": {},
   "source": [
    "## Visualization playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cb4c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize = True\n",
    "\n",
    "if visualize:\n",
    "    if toy_env:\n",
    "        x = torch.load(\"batch_tensor_toy.pt\").to(device)\n",
    "        h = torch.load(\"h_tensor.pt\").to(device)\n",
    "    else:\n",
    "        x = torch.load(\"batch_tensor.pt\")[0].unsqueeze(dim=0).to(device)\n",
    "        h = torch.load(\"h_tensor.pt\").to(device)\n",
    "    \n",
    "    \"\"\" show the observation \"\"\"\n",
    "    plt.imshow(torch.permute(x[0].cpu(), (1,2,0)), cmap=\"gray\")\n",
    "    plt.show()\n",
    "    \n",
    "    \"\"\" show the reconstruction \"\"\"\n",
    "    rssm.vae.eval()\n",
    "    with torch.no_grad():\n",
    "        x_pred = rssm.vae(h, x)\n",
    "        plt.imshow(torch.permute(x_pred[0].cpu(), (1,2,0)), cmap=\"gray\")\n",
    "        plt.show()\n",
    "    rssm.vae.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20995943",
   "metadata": {},
   "outputs": [],
   "source": [
    "rssm.vae.encode(h,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090d8520",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
