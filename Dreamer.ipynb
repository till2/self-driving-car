{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0b1c5dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from collections import deque\n",
    "from operator import itemgetter\n",
    "\n",
    "import gym_donkeycar\n",
    "import gymnasium as gym\n",
    "import imageio\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from IPython.display import display\n",
    "from ipywidgets import HBox, VBox\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from ruamel.yaml import YAML\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.stats import norm\n",
    "from tensorboard import notebook\n",
    "from tensorboard.backend.event_processing.event_accumulator import \\\n",
    "    EventAccumulator\n",
    "from torch import distributions as dist\n",
    "from torch.distributions import Categorical, Normal\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "# suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"gymnasium.spaces.box\") # module=\"gymnasium\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ[\"IMAGEIO_IGNORE_WARNINGS\"] = \"True\"\n",
    "\n",
    "import stable_baselines3 as sb3\n",
    "from gym_donkeycar.envs.donkey_env import DonkeyEnv\n",
    "from gymnasium import spaces\n",
    "from gymnasium.spaces import Box\n",
    "from stable_baselines3 import A2C, PPO, SAC\n",
    "from stable_baselines3.common import env_checker\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "\n",
    "import src\n",
    "from src.actor_critic_discrete import DiscreteActorCritic\n",
    "from src.actor_critic import ContinuousActorCritic\n",
    "from src.blocks import CategoricalStraightThrough, ConvBlock\n",
    "from src.categorical_vae import CategoricalVAE\n",
    "from src.imagination_env import make_imagination_env\n",
    "from src.mlp import MLP\n",
    "from src.preprocessing import transform\n",
    "from src.replay_buffer import ReplayBuffer\n",
    "from src.rssm import RSSM\n",
    "from src.utils import (load_config, make_env, save_image_and_reconstruction,\n",
    "                       to_np, symlog, symexp, twohot_encode, ExponentialMovingAvg,\n",
    "                       ActionExponentialMovingAvg, MetricsTracker)\n",
    "from src.vae import VAE\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Load the config\n",
    "config = load_config()\n",
    "for key in config:\n",
    "    locals()[key] = config[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239558a5",
   "metadata": {},
   "source": [
    "## Init the RSSM (including all networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cda9f099",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing encoder:\n",
      "- adding ConvBlock((3, 64))                   ==> output shape: (64, 32, 32) ==> prod: 65536\n",
      "- adding ConvBlock((64, 128))                   ==> output shape: (128, 16, 16) ==> prod: 32768\n",
      "- adding ConvBlock((128, 256))                   ==> output shape: (256, 8, 8) ==> prod: 16384\n",
      "- adding ConvBlock((256, 512))                   ==> output shape: (512, 4, 4) ==> prod: 8192\n",
      "- adding ConvBlock((512, 256))                   ==> output shape: (256, 2, 2) ==> prod: 1024\n",
      "- adding Flatten()\n",
      "- adding Reshape: (*,1024) => (*,32,32)\n",
      "\n",
      "Initializing decoder:\n",
      "- adding Reshape: (*,1024) => (*,256,2,2)\n",
      "- adding transpose ConvBlock(256, 256)                   ==> output shape: (256, 4, 4) ==> prod: 4096\n",
      "- adding transpose ConvBlock(256, 512)                   ==> output shape: (512, 8, 8) ==> prod: 32768\n",
      "- adding transpose ConvBlock(512, 256)                   ==> output shape: (256, 16, 16) ==> prod: 65536\n",
      "- adding transpose ConvBlock(256, 128)                   ==> output shape: (128, 32, 32) ==> prod: 131072\n",
      "- adding transpose ConvBlock(128, 3)                   ==> output shape: (3, 64, 64) ==> prod: 12288\n",
      "\n",
      "Initializing dynamics_mlp.\n",
      "\n",
      "Initializing reward_mlp.\n",
      "Adding zero weight init to the output layer.\n",
      "\n",
      "Initializing continue_mlp.\n"
     ]
    }
   ],
   "source": [
    "rssm = RSSM()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56fd60e",
   "metadata": {},
   "source": [
    "TODO:\n",
    "\n",
    "required:\n",
    "- use discrete actor critic agent [WIP ]\n",
    "- distributional reward_predictor [ ]\n",
    "\n",
    "optional:\n",
    "(- output batches with rssm)\n",
    "(- train on vector env with batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed72866",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "29dde0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing encoder:\n",
      "- adding ConvBlock((3, 64))                   ==> output shape: (64, 32, 32) ==> prod: 65536\n",
      "- adding ConvBlock((64, 128))                   ==> output shape: (128, 16, 16) ==> prod: 32768\n",
      "- adding ConvBlock((128, 256))                   ==> output shape: (256, 8, 8) ==> prod: 16384\n",
      "- adding ConvBlock((256, 512))                   ==> output shape: (512, 4, 4) ==> prod: 8192\n",
      "- adding ConvBlock((512, 256))                   ==> output shape: (256, 2, 2) ==> prod: 1024\n",
      "- adding Flatten()\n",
      "- adding Reshape: (*,1024) => (*,32,32)\n",
      "\n",
      "Initializing decoder:\n",
      "- adding Reshape: (*,1024) => (*,256,2,2)\n",
      "- adding transpose ConvBlock(256, 256)                   ==> output shape: (256, 4, 4) ==> prod: 4096\n",
      "- adding transpose ConvBlock(256, 512)                   ==> output shape: (512, 8, 8) ==> prod: 32768\n",
      "- adding transpose ConvBlock(512, 256)                   ==> output shape: (256, 16, 16) ==> prod: 65536\n",
      "- adding transpose ConvBlock(256, 128)                   ==> output shape: (128, 32, 32) ==> prod: 131072\n",
      "- adding transpose ConvBlock(128, 3)                   ==> output shape: (3, 64, 64) ==> prod: 12288\n"
     ]
    }
   ],
   "source": [
    "vae = CategoricalVAE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "babaaf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "img_size = config[\"size\"]\n",
    "channels = 1 if config[\"grayscale\"] else 3\n",
    "\n",
    "input_tensor = torch.rand(batch_size, channels, *img_size)*255 # pixel values are in [0, 255]\n",
    "input_tensor = input_tensor.to(config[\"device\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5838f743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 32, 32]), torch.Size([5, 32, 32]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b = vae.encode(input_tensor)\n",
    "a.shape, b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a9061f57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891aa16d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d275c008",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdd6095",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48fbca7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "229f8133",
   "metadata": {},
   "source": [
    "## Create the imagination environment for training the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9a08c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec4e9cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding a TimeLimit wrapper with 16 max imagination episode steps.\n",
      "Adding an AutoReset wrapper.\n",
      "Adding a RescaleActionV0 wrapper. Low: [-1. -1. -1.], High: [1. 1. 1.]\n",
      "Adding a Gymnasium RecordEpisodeStatistics wrapper.\n"
     ]
    }
   ],
   "source": [
    "imagination_env = make_imagination_env(rssm, replay_buffer, render_mode=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a93b68d",
   "metadata": {},
   "source": [
    "## Init the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe1ee1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing agent with 1536 features and 3 actions.\n",
      "Initializing critic.\n",
      "Adding zero weight init to the output layer.\n",
      "Initializing actor.\n",
      "Adding zero weight init to the output layer.\n"
     ]
    }
   ],
   "source": [
    "# agent = PPO(\n",
    "#     policy=\"MlpPolicy\",\n",
    "#     env=imagination_env,\n",
    "#     verbose=verbose,\n",
    "#     tensorboard_log=log_dir,\n",
    "#     gamma=gamma,\n",
    "#     gae_lambda=lam,\n",
    "#     ent_coef=ent_coef,\n",
    "# )\n",
    "\n",
    "agent = DiscreteActorCritic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705a7a7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcae4900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1536])\n",
      "torch.Size([32, 1536])\n"
     ]
    }
   ],
   "source": [
    "sample_instance = torch.randn(config[\"H\"] + config[\"Z\"]).to(config[\"device\"]) # torch.Size([1536])\n",
    "sample_batch = torch.randn(32, config[\"H\"] + config[\"Z\"]).to(config[\"device\"]) # torch.Size([32, 1536])\n",
    "\n",
    "print(sample_instance.shape)\n",
    "print(sample_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "672c8e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af9ae06c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = torch.randn(batch_size, config[\"A\"]).to(config[\"device\"]) # torch.Size([3])\n",
    "action.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e8ae331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 512])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = torch.randn(batch_size, config[\"H\"]).to(config[\"device\"]) # torch.Size([512])\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10c8fde4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1024])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.randn(batch_size, config[\"Z\"]).to(config[\"device\"]) # torch.Size([1024]) = 32*32\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96e3dca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "h, reward_pred, continue_prob, continue_pred, x_reconstruction = rssm.step(action, h, z)\n",
    "print(h.shape)\n",
    "print(reward_pred.shape)\n",
    "print(continue_prob.shape)\n",
    "print(continue_pred.shape)\n",
    "print(x_reconstruction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dddcc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27455aa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02b6516",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a59fc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d75d589",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833a4b30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60016e18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e225e265",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5061107",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1396668c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b05696",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae902e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5697097a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c802872",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "856cb39a",
   "metadata": {},
   "source": [
    "## Training loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6367c5ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ae41bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent_tracker = MetricsTracker(\n",
    "#     training_metrics=[\"critic_loss\", \"actor_loss\"],\n",
    "#     episode_metrics=[\"rewards\", \"log_probs\", \"value_preds\", \"critic_dists\", \"entropies\", \"masks\"],\n",
    "# )\n",
    "# \n",
    "# for imagination_phase in range(agent_update_phases_per_model_update):\n",
    "#     \n",
    "#     if imagination_phase == 0:\n",
    "#         dream_obs, dream_info = imagination_env.reset(seed=42)\n",
    "#         dream_obs = torch.tensor(dream_obs).to(device)\n",
    "#     \n",
    "#     for step in range(imagination_steps_per_agent_update):\n",
    "#         \n",
    "#         value_pred, critic_dist = agent.apply_critic(dream_obs)\n",
    "# \n",
    "#         # Get an action and take an environment step\n",
    "#         action, log_prob, actor_entropy = agent.get_action(dream_obs)\n",
    "#         dream_obs, reward, terminated, truncated, info = imagination_env.step(to_np(action))\n",
    "#         \n",
    "#         # Transform the next obs\n",
    "#         dream_obs = torch.Tensor(dream_obs).to(device)\n",
    "#         \n",
    "#         # every step:\n",
    "#         agent_tracker.add(\n",
    "#             episode_metrics={\n",
    "#                 \"rewards\": reward,\n",
    "#                 \"log_probs\": log_prob,\n",
    "#                 \"value_preds\": value_pred,\n",
    "#                 \"critic_dists\": critic_dist,\n",
    "#                 \"entropies\": actor_entropy,\n",
    "#                 \"masks\": torch.tensor(0.0 if terminated else 1.0),\n",
    "#             }\n",
    "#         )\n",
    "#     \n",
    "#     # every imagination phase:\n",
    "#     episode_batches = agent_tracker.get_episode_batches() # episode_batches is a dict\n",
    "#     last_value_pred, _ = agent.apply_critic(dream_obs) # last value prediction for GAE\n",
    "# \n",
    "#     # Update the agent's parameters\n",
    "#     critic_loss, actor_loss = agent.get_loss(episode_batches, last_value_pred)\n",
    "#     agent.update_parameters(critic_loss, actor_loss)\n",
    "#     \n",
    "#     # Only for the last imagined episode: Log the episode metrics\n",
    "#     if imagination_phase == agent_update_phases_per_model_update - 1:\n",
    "# \n",
    "#         agent_tracker.add(\n",
    "#             training_metrics={\n",
    "#                 \"critic_loss\": critic_loss,\n",
    "#                 \"actor_loss\": actor_loss,\n",
    "#             }\n",
    "#         )\n",
    "#     \n",
    "#         # Episode return\n",
    "#         if len(imagination_env.return_queue):\n",
    "#             agent_tracker.writer.add_scalar(\"dream_episode_return\", np.array(imagination_env.return_queue)[-1], global_step=sample_phase)\n",
    "#     \n",
    "#         # Actor and critic losses\n",
    "#         agent_tracker.log_to_tensorboard(step=sample_phase)\n",
    "#     \n",
    "#         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bff31322",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating environment with properties:\n",
      "- type: toy env\n",
      "- adding a TimeLimit wrapper with 1000 max episode steps\n",
      "- adding an AutoReset wrapper\n",
      "- making a AsyncVectorEnv with 16 envs\n",
      "- adding a RescaleActionV0 wrapper Low: -1, High: 1\n",
      "- adding a Gymnasium RecordEpisodeStatistics wrapper\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▍                                                                                      | 43/8812 [01:10<3:58:10,  1.63s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [13], line 79\u001b[0m\n\u001b[1;32m     76\u001b[0m h \u001b[38;5;241m=\u001b[39m rssm\u001b[38;5;241m.\u001b[39mstep(action, h, z)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Take an environment step with the action\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mto_np\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m x \u001b[38;5;241m=\u001b[39m transform(torch\u001b[38;5;241m.\u001b[39mtensor(obs)) \u001b[38;5;66;03m# => (B,C,H,W)\u001b[39;00m\n\u001b[1;32m     81\u001b[0m replay_buffer\u001b[38;5;241m.\u001b[39mpush(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/wrappers/record_episode_statistics.py:89\u001b[0m, in \u001b[0;36mRecordEpisodeStatistics.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;124;03m\"\"\"Steps through the environment, recording the episode statistics.\"\"\"\u001b[39;00m\n\u001b[1;32m     83\u001b[0m     (\n\u001b[1;32m     84\u001b[0m         observations,\n\u001b[1;32m     85\u001b[0m         rewards,\n\u001b[1;32m     86\u001b[0m         terminations,\n\u001b[1;32m     87\u001b[0m         truncations,\n\u001b[1;32m     88\u001b[0m         infos,\n\u001b[0;32m---> 89\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m     91\u001b[0m         infos, \u001b[38;5;28mdict\u001b[39m\n\u001b[1;32m     92\u001b[0m     ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`info` dtype is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(infos)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m while supported dtype is `dict`. This may be due to usage of other wrappers in the wrong order.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_returns \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m rewards\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/core.py:538\u001b[0m, in \u001b[0;36mActionWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    535\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[1;32m    536\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;124;03m\"\"\"Runs the :attr:`env` :meth:`env.step` using the modified ``action`` from :meth:`self.action`.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/vector/vector_env.py:203\u001b[0m, in \u001b[0;36mVectorEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03m\"\"\"Take an action for each parallel environment.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03m    {}\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 203\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/vector/async_vector_env.py:321\u001b[0m, in \u001b[0;36mAsyncVectorEnv.step_wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    319\u001b[0m successes \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, pipe \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent_pipes):\n\u001b[0;32m--> 321\u001b[0m     result, success \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m     successes\u001b[38;5;241m.\u001b[39mappend(success)\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m success:\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py:255\u001b[0m, in \u001b[0;36m_ConnectionBase.recv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 255\u001b[0m buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _ForkingPickler\u001b[38;5;241m.\u001b[39mloads(buf\u001b[38;5;241m.\u001b[39mgetbuffer())\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py:419\u001b[0m, in \u001b[0;36mConnection._recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_recv_bytes\u001b[39m(\u001b[38;5;28mself\u001b[39m, maxsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 419\u001b[0m     buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    420\u001b[0m     size, \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39munpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!i\u001b[39m\u001b[38;5;124m\"\u001b[39m, buf\u001b[38;5;241m.\u001b[39mgetvalue())\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py:384\u001b[0m, in \u001b[0;36mConnection._recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    382\u001b[0m remaining \u001b[38;5;241m=\u001b[39m size\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m remaining \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 384\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m \u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    385\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk)\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# New dreamer training loop with batches for the distributional critic\n",
    "# (and later: add manual actor-critic training in imagination env. and delete sb3).\n",
    "# add batches to rssm\n",
    "\n",
    "### rssm.load_weights(\"weights/RSSM_1.70111713\")\n",
    "### rssm.train()\n",
    "\n",
    "# Create the environment\n",
    "env = make_env()\n",
    "batch_size = config[\"n_envs\"]\n",
    "\n",
    "# World Model Logging\n",
    "model_tracker = MetricsTracker(\n",
    "    # Log the mean loss for the training metrics\n",
    "    training_metrics=[\"loss\", \"image_loss\", \"reward_loss\", \"continue_loss\", \"dyn_loss\", \"rep_loss\", \"rewards\"],\n",
    "    \n",
    "    # Loss per step\n",
    "    episode_metrics=[\"loss\", \"image_loss\", \"reward_loss\", \"continue_loss\", \"dyn_loss\", \"rep_loss\", \"rewards\"],\n",
    ")\n",
    "\n",
    "# Agent Logging\n",
    "agent_tracker = MetricsTracker(\n",
    "    training_metrics=[\"critic_loss\", \"actor_loss\"],\n",
    "    episode_metrics=[\"rewards\", \"log_probs\", \"value_preds\", \"critic_dists\", \"entropies\", \"masks\"],\n",
    ")\n",
    "\n",
    "\n",
    "for sample_phase in tqdm(range(start_phase, n_seed_phases + n_model_updates)):\n",
    "    \n",
    "    # Reset the RNN's hidden state\n",
    "    ### h = torch.zeros(rssm.num_rnn_layers, 1, H, device=device, dtype=torch.float32) # seq_len, B, H\n",
    "    # NEW:\n",
    "    # - init in rssm.step (if h is not given)\n",
    "    \n",
    "    if sample_phase == start_phase:\n",
    "        \n",
    "        # Get the first obs\n",
    "        obs, info = env.reset(seed=42)\n",
    "        x = transform(torch.tensor(obs)) # => (B,C,H,W)\n",
    "        replay_buffer.push(x)\n",
    "        \n",
    "        # NEW:\n",
    "        step_dict = rssm.pre_step(x=x)\n",
    "    \n",
    "    for step in range(n_steps_per_model_update):\n",
    "    \n",
    "        \"\"\" WORLD MODEL LEARNING \"\"\"\n",
    "\n",
    "        # Get real z and z prediction, also get state\n",
    "        ###z_prior = rssm.dynamics_mlp(h).view(-1, num_categoricals, num_classes) # (1,32,32) for the softmax\n",
    "        ###z_prior = F.softmax(z_prior, -1).flatten(start_dim=1, end_dim=2) # (1, 1024)\n",
    "        ###z = rssm.vae.encode(x).flatten(start_dim=1, end_dim=2)\n",
    "        ###state = to_np(torch.cat((h.flatten().detach(), z.flatten().detach()), dim=0))\n",
    "\n",
    "        # Predict one step using the world model (for training it)\n",
    "        ### h, reward_pred, continue_prob, continue_pred, x_pred = rssm.step(action, h, z)\n",
    "        # NEW:\n",
    "        h = step_dict[\"h\"]\n",
    "        step_dict = rssm.pre_step(h, x)\n",
    "        assert h.shape == torch.Size([batch_size, H])\n",
    "        assert x.shape == torch.Size([batch_size, 1 if grayscale else 3, *size])\n",
    "        \n",
    "        #for key, val in step_dict.items():\n",
    "        #    print(f\"{key} shape:\", val.shape)\n",
    "        \n",
    "        # Get action (random in init phases, otherwise from agent)\n",
    "        if sample_phase < n_seed_phases:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            assert step_dict[\"state\"].shape == torch.Size([batch_size, H+Z])\n",
    "            action, _, _ = agent.get_action(step_dict[\"state\"])\n",
    "        \n",
    "        # Update the world model state\n",
    "        h = step_dict[\"h\"]\n",
    "        z = step_dict[\"z\"]\n",
    "        h = rssm.step(action, h, z)\n",
    "\n",
    "        # Take an environment step with the action\n",
    "        obs, reward, terminated, truncated, info = env.step(to_np(action))\n",
    "        x = transform(torch.tensor(obs)) # => (B,C,H,W)\n",
    "        replay_buffer.push(x)\n",
    "\n",
    "        # Calculate the world model loss\n",
    "        ### continue_target = torch.tensor(1 - (terminated | truncated), device=device, dtype=torch.float32)\n",
    "        ### reward = torch.tensor(reward, device=device, dtype=torch.float32)\n",
    "        ###losses = rssm.get_losses(x, x_pred, reward, reward_pred, \n",
    "        ###                         continue_target, continue_prob, z_prior, z)\n",
    "        # NEW:\n",
    "        step_dict[\"continue_target\"] = continue_target = torch.tensor(1 - (terminated | truncated), device=device, dtype=torch.float32)\n",
    "        step_dict[\"reward_target\"] = reward = torch.tensor(reward, device=device, dtype=torch.float32)\n",
    "        \n",
    "        losses = rssm.get_losses(step_dict)\n",
    "\n",
    "        # Track the losses\n",
    "        model_tracker.add(\n",
    "            episode_metrics=losses # losses is a dict with batches for all episode metrics\n",
    "        )\n",
    "        # Track the reward for the episode return\n",
    "        model_tracker.add(\n",
    "            episode_metrics={\n",
    "                \"rewards\": reward,\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    # Get mean loss and update world model\n",
    "    episode_losses = model_tracker.get_episode_batches(reduction=\"mean\") # episode_losses is a dict\n",
    "    rssm.update_parameters(episode_losses[\"loss\"])\n",
    "\n",
    "    \n",
    "    \"\"\" RL AGENT LEARNING (IN THE WORLD MODEL) \"\"\"\n",
    "    if verbose and sample_phase == n_seed_phases:\n",
    "        print(\"The agent starts learning.\")\n",
    "\n",
    "        \n",
    "    if sample_phase >= n_seed_phases:\n",
    "        ### agent.learn(\n",
    "        ###     total_timesteps=imagination_steps_per_model_update,\n",
    "        ###     progress_bar=imagination_progress_bar,\n",
    "        ###     reset_num_timesteps=False\n",
    "        ### )\n",
    "        \n",
    "        # NEW: discrete actor critic\n",
    "        for imagination_phase in range(agent_update_phases_per_model_update):\n",
    "            print(\"IMAGINATION PHASE:\", imagination_phase) # debug\n",
    "            \n",
    "            if imagination_phase == 0:\n",
    "                dream_obs, dream_info = imagination_env.reset(seed=42)\n",
    "                dream_obs = torch.tensor(dream_obs).to(device)\n",
    "\n",
    "            for step in range(imagination_steps_per_agent_update):\n",
    "\n",
    "                value_pred, critic_dist = agent.apply_critic(dream_obs)\n",
    "\n",
    "                # Get an action and take an environment step\n",
    "                action, log_prob, actor_entropy = agent.get_action(dream_obs)\n",
    "                dream_obs, reward, terminated, truncated, info = imagination_env.step(to_np(action))\n",
    "\n",
    "                # Transform the next obs\n",
    "                dream_obs = torch.Tensor(dream_obs).to(device)\n",
    "\n",
    "                # every step:\n",
    "                agent_tracker.add(\n",
    "                    episode_metrics={\n",
    "                        \"rewards\": reward,\n",
    "                        \"log_probs\": log_prob,\n",
    "                        \"value_preds\": value_pred,\n",
    "                        \"critic_dists\": critic_dist,\n",
    "                        \"entropies\": actor_entropy,\n",
    "                        \"masks\": torch.tensor(0.0 if terminated else 1.0),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            # every imagination phase:\n",
    "            episode_batches = agent_tracker.get_episode_batches() # episode_batches is a dict\n",
    "            last_value_pred, _ = agent.apply_critic(dream_obs) # last value prediction for GAE\n",
    "\n",
    "            # Update the agent's parameters\n",
    "            critic_loss, actor_loss = agent.get_loss(episode_batches, last_value_pred)\n",
    "            agent.update_parameters(critic_loss, actor_loss)\n",
    "\n",
    "            # Only for the last imagined episode: Log the episode metrics\n",
    "            print(imagination_phase, agent_update_phases_per_model_update-1)\n",
    "            if imagination_phase == agent_update_phases_per_model_update - 1:\n",
    "                print(\"HI\")\n",
    "\n",
    "                agent_tracker.add(\n",
    "                    training_metrics={\n",
    "                        \"critic_loss\": critic_loss,\n",
    "                        \"actor_loss\": actor_loss,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                # Episode return\n",
    "                if len(imagination_env.return_queue):\n",
    "                    agent_tracker.writer.add_scalar(\"dream_episode_return\", np.array(imagination_env.return_queue)[-1], global_step=sample_phase)\n",
    "\n",
    "                # Actor and critic losses\n",
    "                agent_tracker.log_to_tensorboard(step=sample_phase)\n",
    "\n",
    "    \n",
    "    # Every couple episodes:\n",
    "    if sample_phase % config[\"log_interval\"] == 0:\n",
    "        \n",
    "        # Log: mean episode losses\n",
    "        model_tracker.add(\n",
    "            training_metrics=episode_losses\n",
    "        )\n",
    "        \n",
    "        # Log: episode return\n",
    "        if len(env.return_queue):\n",
    "            model_tracker.writer.add_scalar(\"episode_return\", np.array(env.return_queue)[-1], global_step=len(env.return_queue))\n",
    "        \n",
    "        # Tensorboard\n",
    "        model_tracker.log_to_tensorboard(step=sample_phase)\n",
    "        \n",
    "        # TODO Later: actor and critic losses\n",
    "\n",
    "        # Save original image and reconstruction\n",
    "        save_image_and_reconstruction(step_dict[\"x\"], step_dict[\"x_reconstruction\"], sample_phase)\n",
    "\n",
    "env.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b98c138f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(batch_size, 64,64,3).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6659d476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 envs: 1.2/it\n",
    "# 8 envs: 1.4/it\n",
    "# 16 envs: 1.6/it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d254602",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7718278a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(to_np(step_dict[\"x_reconstruction\"][0].permute(1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974d6e1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a4e43e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cbc339",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a22ad1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ce5b97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98698051",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c67441",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f41fa6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3c5652",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b67bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_dict[\"x\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558d9e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_dict[\"reward_target\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6ee982",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_dict[\"reward_target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b9ab3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 7\n",
    "h = torch.randn(batch_size, config[\"H\"]).to(config[\"device\"])\n",
    "x = torch.randn(batch_size, 3, 64, 64).to(config[\"device\"]) #  (B, C, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6aa3968",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape[2:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9169d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(env.reset()[0][0]).shape\n",
    "transform(torch.tensor(env.reset()[0][0])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad8f4ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690d9572",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "step_dict = rssm.pre_step(x=x)\n",
    "step_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7896396",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_dict[\"z_pred\"] == step_dict[\"z\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5169db30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3816c365",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaff2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = torch.zeros(batch_size, H)\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946bd69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PPO(\n",
    "policy=\"MlpPolicy\",\n",
    "env=imagination_env,\n",
    "verbose=verbose,\n",
    "tensorboard_log=log_dir,\n",
    "gamma=gamma,\n",
    "gae_lambda=lam,\n",
    "ent_coef=ent_coef).predict(state, deterministic=False)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd303c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "action.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04d9e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "action.squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb66562",
   "metadata": {},
   "outputs": [],
   "source": [
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7823a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5d62a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat((action.unsqueeze(0), h, z), 1).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c14715",
   "metadata": {},
   "outputs": [],
   "source": [
    "CategoricalStraightThrough.forward?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a3a48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "# - training loop\n",
    "# except KeyboardInterrupt:\n",
    "#     \"\"\" Clean handling for interrupts to stop training early \"\"\"\n",
    "#     print(\"Stopping training.\")\n",
    "#     # Delete the last loss if the training was stopped early\n",
    "#     # so that the list only consists of floats\n",
    "#     for key in episode_losses:\n",
    "#         if isinstance(episode_losses[key][-1], torch.Tensor):\n",
    "#             episode_losses[key] = episode_losses[key][:-1]\n",
    "# \n",
    "#     # Close the TensorBoard writer and the gym environment\n",
    "#     writer.close()\n",
    "#     env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ad0c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#            # save the rssm and agent\n",
    "#            rssm.save_weights(filename=f\"RSSM_{best_running_loss:.8f}\")\n",
    "#            agent.save(f\"weights/{agent.__class__.__name__}_agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d96d812",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a2fb86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42fb0f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7ebb9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90979190",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ea7a23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82409e2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55e74d14",
   "metadata": {},
   "source": [
    "## Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0703ff02",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results = False\n",
    "\n",
    "if plot_results:\n",
    "    rolling_length = max(1, int(len(episode_losses[\"episode_loss\"])/20))\n",
    "\n",
    "    fig, axs = plt.subplots(ncols=3, nrows=2, figsize=(3*5, 2*5))\n",
    "\n",
    "    # Iterate over the keys and plot the losses\n",
    "    for i, key in enumerate(episode_losses.keys()):\n",
    "        row = i // 3\n",
    "        col = i % 3\n",
    "\n",
    "        axs[row, col].set_title(key)\n",
    "        losses = episode_losses[key]\n",
    "        losses_moving_average = (\n",
    "            np.convolve(\n",
    "                np.array(losses).flatten(), np.ones(rolling_length), mode=\"valid\"\n",
    "            )\n",
    "            / rolling_length\n",
    "        )\n",
    "        axs[row, col].plot(range(len(losses)), losses, label=key)\n",
    "        axs[row, col].plot(range(len(losses_moving_average)), losses_moving_average, label=\"moving average\")\n",
    "        axs[row, col].legend(loc=\"upper right\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d0fd15",
   "metadata": {},
   "source": [
    "## Showcase the trained agent playing in latent imagination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f62546",
   "metadata": {},
   "outputs": [],
   "source": [
    "showcase_agent = False\n",
    "\n",
    "if showcase_agent:\n",
    "    \n",
    "    showcase_rewards = []\n",
    "    imagination_env.render_mode = \"gif\"\n",
    "    obs, info = imagination_env.reset()\n",
    "    \n",
    "    for i in range(500):\n",
    "        \n",
    "        # apply the RL agent in eval mode to get an action\n",
    "        state = to_np(torch.cat((h.flatten().detach(), z.flatten().detach()), dim=0))\n",
    "        action, _ = agent.predict(state, deterministic=False)\n",
    "        # action = imagination_env.action_space.sample()\n",
    "        \n",
    "        obs, reward, terminated, truncated, info = imagination_env.step(action)\n",
    "        showcase_rewards.append(reward)\n",
    "        imagination_env.render()\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            break\n",
    "        \n",
    "    imagination_env.close()\n",
    "    imagination_env.render_mode = None\n",
    "\n",
    "    plt.plot(showcase_rewards)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e073982",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3354e868",
   "metadata": {},
   "source": [
    "## Test area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cb6387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tensorboard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35914cfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f70fa43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bdcc84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
