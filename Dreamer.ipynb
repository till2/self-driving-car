{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b1c5dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gym_donkeycar\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import HBox, VBox\n",
    "from IPython.display import display\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal, Categorical\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "# custom classes and functions\n",
    "import models\n",
    "from models.blocks import ConvBlock, TransposeConvBlock, ResConvBlock, CategoricalStraightThrough\n",
    "from models.mlp import MLP\n",
    "from models.autoencoder import Autoencoder\n",
    "from models.vae import VAE\n",
    "from models.categorical_vae import CategoricalVAE\n",
    "from preprocessing import grayscale_transform as transform\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa538bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf2df7b",
   "metadata": {},
   "source": [
    "## Create VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b664daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| CategoricalVAE info |\n",
      "-----------------------\n",
      "device: cuda\n",
      "number of parameters: 38_151_857\n",
      "input shape : [8, 1, 128, 128]\n",
      "hidden shape: [8, 32, 32]\n",
      "output shape: [8, 1, 128, 128]\n",
      "vae_ent_coeff: 1e-08\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "\n",
    "# VAE\n",
    "vae = CategoricalVAE(\n",
    "    greyscale=True,\n",
    "    vae_ent_coeff=1e-8\n",
    ").to(device)\n",
    "\n",
    "# VAE optimizer\n",
    "vae_optim = optim.Adam(\n",
    "    vae.parameters(), \n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-5 # l2 regularization\n",
    ")\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group[\"lr\"]\n",
    "\n",
    "# display VAE stats\n",
    "vae.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239558a5",
   "metadata": {},
   "source": [
    "## RSSM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4616b784",
   "metadata": {},
   "source": [
    "```py\n",
    "\n",
    "- given: h, obs (obs is given only in training mode)\n",
    "\n",
    "# predict z from h\n",
    "z_pred = dynamics_mlp(h)\n",
    "\n",
    "# get true z from encoder\n",
    "z_sample = vae.encode(obs) # returns one-hot on softmax sample\n",
    "\n",
    "state = concat(h,z)\n",
    "\n",
    "# apply actor and critic nets on state\n",
    "v = value_net(state)\n",
    "a = actor_critic(state) # a is a 2x1 vector\n",
    "\n",
    "# predict other stuff\n",
    "r = reward_mlp(state)\n",
    "c = continue_mlp(state) # binary classification\n",
    "\n",
    "# combine everything into belief state\n",
    "x = concat(a, state)\n",
    "\n",
    "\n",
    "# apply rnn\n",
    "h_new = rnn(x, h)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0560ba3",
   "metadata": {},
   "source": [
    "## Create RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8c78302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN ==> number of parameters: 3_153_408\n"
     ]
    }
   ],
   "source": [
    "A = 3 # number of action dimensions (3 for toy, 2 for real sim)\n",
    "H = 512 # discrete h state \n",
    "Z = 32*32 # stochastic Zx1 state\n",
    "num_rnn_layers = 1\n",
    "\n",
    "rnn = nn.GRU(\n",
    "    input_size=A+H+Z,\n",
    "    hidden_size=H,\n",
    "    num_layers=num_rnn_layers\n",
    ").to(device)\n",
    "\n",
    "h = torch.zeros(num_rnn_layers, 1, H) # seq_len, B, H\n",
    "\n",
    "print(f\"RNN ==> number of parameters: {sum(p.numel() for p in rnn.parameters() if p.requires_grad):_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc178dac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "519ec6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamics_mlp = MLP(input_dims=H, output_dims=Z).to(device) # H -> Z\n",
    "reward_mlp = MLP(input_dims=H+Z, output_dims=1).to(device) # state (H+Z) -> 1\n",
    "continue_mlp = MLP(input_dims=H+Z, output_dims=1).to(device) # state (H+Z)->1 # put sigmoid and binary-crossentropy on top\n",
    "\n",
    "value_net = MLP(input_dims=H+Z, output_dims=1).to(device) # state (H+Z) -> 1\n",
    "policy_net = MLP(input_dims=H+Z, output_dims=A).to(device) # state (H+Z) -> A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856cb39a",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1cd076d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train episode [0/100] ===> Loss: 0.738, ReconstructionLoss: 0.738, weighted_entropy_loss: -0.000, lr: 0.001\n",
      "train episode [1/100] ===> Loss: 0.740, ReconstructionLoss: 0.740, weighted_entropy_loss: -0.000, lr: 0.001\n",
      "train episode [2/100] ===> Loss: 0.735, ReconstructionLoss: 0.735, weighted_entropy_loss: -0.000, lr: 0.001\n",
      "train episode [3/100] ===> Loss: 0.735, ReconstructionLoss: 0.735, weighted_entropy_loss: -0.000, lr: 0.001\n",
      "train episode [4/100] ===> Loss: 0.728, ReconstructionLoss: 0.728, weighted_entropy_loss: -0.000, lr: 0.001\n",
      "train episode [5/100] ===> Loss: 0.729, ReconstructionLoss: 0.729, weighted_entropy_loss: -0.000, lr: 0.001\n",
      "train episode [6/100] ===> Loss: 0.746, ReconstructionLoss: 0.746, weighted_entropy_loss: -0.000, lr: 0.001\n",
      "train episode [7/100] ===> Loss: 0.729, ReconstructionLoss: 0.729, weighted_entropy_loss: -0.000, lr: 0.001\n",
      "train episode [8/100] ===> Loss: 0.739, ReconstructionLoss: 0.739, weighted_entropy_loss: -0.000, lr: 0.001\n",
      "train episode [9/100] ===> Loss: 0.736, ReconstructionLoss: 0.736, weighted_entropy_loss: -0.000, lr: 0.001\n",
      "train episode [10/100] ===> Loss: 0.738, ReconstructionLoss: 0.738, weighted_entropy_loss: -0.000, lr: 0.001\n",
      "train episode [11/100] ===> Loss: 0.741, ReconstructionLoss: 0.741, weighted_entropy_loss: -0.000, lr: 0.001\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [24], line 62\u001b[0m\n\u001b[1;32m     60\u001b[0m vae_optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     61\u001b[0m vae_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 62\u001b[0m \u001b[43mvae_optim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m vae_losses\u001b[38;5;241m.\u001b[39mappend(vae_loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     65\u001b[0m reconstruction_losses\u001b[38;5;241m.\u001b[39mappend(reconstruction_loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:113\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:157\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    153\u001b[0m                 max_exp_avg_sqs\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_exp_avg_sq\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    155\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 157\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m         \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m         \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m         \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m         \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m         \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m         \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m         \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:213\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 213\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:263\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    262\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mmul_(beta1)\u001b[38;5;241m.\u001b[39madd_(grad, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[0;32m--> 263\u001b[0m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable:\n\u001b[1;32m    266\u001b[0m     step \u001b[38;5;241m=\u001b[39m step_t\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\" vae training loop \"\"\"\n",
    "\n",
    "# create the environment\n",
    "toy_env = True\n",
    "\n",
    "if toy_env:\n",
    "    assert A==3\n",
    "    env = gym.make(\"CarRacing-v2\", max_episode_steps=40, render_mode=\"rgb_array\")\n",
    "else:\n",
    "    assert A==2\n",
    "    exe_path = \"/home/till/Desktop/Thesis/donkeycar_sim/DonkeySimLinux/donkey_sim.x86_64\"\n",
    "    port = 9091\n",
    "    config = {\n",
    "        \"exe_path\" : exe_path, \n",
    "        \"port\" : port \n",
    "    }\n",
    "    env = gym.make(\n",
    "        \"GymV21Environment-v0\", \n",
    "        env_id=\"donkey-minimonaco-track-v0\", # donkey-warehouse-v0 \n",
    "        make_kwargs={\n",
    "            \"conf\": config\n",
    "        })\n",
    "\n",
    "verbose = True\n",
    "n_episodes = 100\n",
    "vae_losses, reconstruction_losses, entropy_losses = [], [], []\n",
    "vae.train()\n",
    "\n",
    "for episode in range(n_episodes): # tqdm\n",
    "    \n",
    "    # get the initial state\n",
    "    obs, info = env.reset()\n",
    "    \n",
    "    # reset the rnn's hidden state\n",
    "    h = torch.zeros(num_rnn_layers, 1, H, device=device) # seq_len, B, H\n",
    "    \n",
    "    # setup a minibatch of x's for training the autoencoder\n",
    "    batch_counter = 0\n",
    "    batch_tensor = torch.empty((batch_size,) + transform(obs).shape, device=device) # B,C,H,W\n",
    "    \n",
    "    # play one episode\n",
    "    done = False\n",
    "    while not done:\n",
    "             \n",
    "        # add the new x to the batch\n",
    "        x = transform(obs).view(-1, 1, 128, 128)\n",
    "        batch_tensor[batch_counter] = transform(obs)\n",
    "        batch_counter += 1\n",
    "        \n",
    "        \"\"\" train networks on accumulated batch \"\"\"\n",
    "        if batch_counter % batch_size == 0:\n",
    "            # reset the batch counter\n",
    "            batch_counter = 0\n",
    "            \n",
    "            # autoencoder forward pass with a minibatch\n",
    "            xhat = vae(batch_tensor)\n",
    "\n",
    "            # get a loss and update the autoencoder\n",
    "            vae_loss, reconstruction_loss, weighted_entropy_loss = vae.get_loss(batch_tensor, xhat)\n",
    "            vae_optim.zero_grad()\n",
    "            vae_loss.backward()\n",
    "            vae_optim.step()\n",
    "            \n",
    "            vae_losses.append(vae_loss.item())\n",
    "            reconstruction_losses.append(reconstruction_loss.item())\n",
    "            entropy_losses.append(weighted_entropy_loss.item())\n",
    "        \n",
    "        # predict z from h\n",
    "        z_pred = dynamics_mlp(h).view(-1, Z) # B,1024 (flattened)\n",
    "        \n",
    "        # get true z from encoder\n",
    "        z = vae.encode(x).view(-1, Z) # B,1024 (flattened)\n",
    "        \n",
    "        state = torch.cat((h.view(-1, H), z), 1) # 1 (B), H+Z\n",
    "        \n",
    "        # apply actor and critic nets on state\n",
    "        v = value_net(state)\n",
    "        action = policy_net(state) # a is a 2x1 vector\n",
    "        \n",
    "        # predict other stuff\n",
    "        r = reward_mlp(state)\n",
    "        c = continue_mlp(state) # binary classification\n",
    "        \n",
    "        # combine everything and apply the rnn to get the next hidden state\n",
    "        rnn_input = torch.cat((action, state), dim=1)\n",
    "        _, h = rnn(rnn_input, h.view(-1, H))\n",
    "        \n",
    "        # choose and execute an action\n",
    "        # action = env.action_space.sample()\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action.squeeze().detach().cpu().numpy())        \n",
    "               \n",
    "        done = terminated or truncated\n",
    "        obs = next_obs\n",
    "    \n",
    "    if verbose: \n",
    "        print(f\"train episode [{episode}/{n_episodes}] ===> Loss: {vae_loss.item():.3f}, ReconstructionLoss: {reconstruction_loss.item():.3f}, weighted_entropy_loss: {weighted_entropy_loss.item():.3f}, lr: {get_lr(vae_optim)}\")\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e74d14",
   "metadata": {},
   "source": [
    "## Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8124082",
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_length = 1\n",
    "\n",
    "fig, axs = plt.subplots(ncols=3, figsize=(12, 5))\n",
    "\n",
    "axs[0].set_title(\"VAE Loss\")\n",
    "vae_losses_moving_average = (\n",
    "    np.convolve(\n",
    "        np.array(vae_losses).flatten(), np.ones(rolling_length), mode=\"valid\"\n",
    "    )\n",
    "    / rolling_length\n",
    ")\n",
    "axs[0].plot(range(len(vae_losses_moving_average)), vae_losses_moving_average)\n",
    "\n",
    "\n",
    "axs[1].set_title(\"Reconstruction Loss\")\n",
    "reconstruction_losses_moving_average = (\n",
    "    np.convolve(\n",
    "        np.array(reconstruction_losses).flatten(), np.ones(rolling_length), mode=\"valid\"\n",
    "    )\n",
    "    / rolling_length\n",
    ")\n",
    "axs[1].plot(range(len(reconstruction_losses_moving_average)), reconstruction_losses_moving_average)\n",
    "\n",
    "axs[2].set_title(\"Weighted Entropy Loss\")\n",
    "entropy_losses_moving_average = (\n",
    "    np.convolve(\n",
    "        np.array(entropy_losses).flatten(), np.ones(rolling_length), mode=\"valid\"\n",
    "    )\n",
    "    / rolling_length\n",
    ")\n",
    "axs[2].plot(range(len(entropy_losses_moving_average)), entropy_losses_moving_average);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07793b6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c8481d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30768921",
   "metadata": {},
   "source": [
    "# Overfitting and visualization playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d510187a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" overfit to one sample \"\"\"\n",
    "batch_tensor = torch.load(\"batch_tensor.pt\").to(device)\n",
    "vae_losses, reconstruction_losses, entropy_losses = [], [], []\n",
    "\n",
    "for i in range(200):\n",
    "    \n",
    "    xhat = vae(batch_tensor)\n",
    "\n",
    "    # get a loss and update the autoencoder\n",
    "    vae_loss, reconstruction_loss, entropy_loss = vae.get_loss(batch_tensor, xhat)\n",
    "    vae_optim.zero_grad()\n",
    "    vae_loss.backward()\n",
    "    vae_optim.step()\n",
    "\n",
    "    vae_losses.append(vae_loss.item())\n",
    "    reconstruction_losses.append(reconstruction_loss.item())\n",
    "    entropy_losses.append(entropy_loss.item())\n",
    "\n",
    "    if i % 20 == 0:\n",
    "        print(f\"[{i+1}/{200}] loss: {vae_loss.item()}, entropy loss: {entropy_loss}, lr: {get_lr(vae_optim)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cb4c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_tensor = torch.load(\"batch_tensor.pt\").to(device)\n",
    "\n",
    "\"\"\" show the observation \"\"\"\n",
    "plt.imshow(torch.permute(batch_tensor[0].cpu(), (1,2,0)), cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "\"\"\" show the reconstruction \"\"\"\n",
    "vae.eval()\n",
    "with torch.no_grad():\n",
    "    xhat = vae(batch_tensor)\n",
    "    plt.imshow(torch.permute(xhat[0].cpu(), (1,2,0)), cmap=\"gray\")\n",
    "    plt.show()\n",
    "vae.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e54b07f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
