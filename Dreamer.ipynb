{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1c5dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gym_donkeycar\n",
    "\n",
    "import os\n",
    "from ruamel.yaml import YAML\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import HBox, VBox\n",
    "from IPython.display import display\n",
    "from tqdm import tqdm\n",
    "\n",
    "from PIL import Image\n",
    "import imageio\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import distributions as dist\n",
    "from torch.distributions import Normal, Categorical\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from tensorboard import notebook\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "\n",
    "# suppress warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ[\"IMAGEIO_IGNORE_WARNINGS\"] = \"True\"\n",
    "\n",
    "from networks.utils import to_np, load_config, save_image_and_reconstruction\n",
    "\n",
    "# custom classes and functions\n",
    "from networks.blocks import ConvBlock, CategoricalStraightThrough\n",
    "from networks.rssm import RSSM\n",
    "from networks.mlp import MLP\n",
    "from networks.categorical_vae import CategoricalVAE\n",
    "from networks.actor_critic import ContinuousActorCritic\n",
    "from preprocessing import grayscale_transform as transform\n",
    "\n",
    "###\n",
    "\n",
    "import stable_baselines3 as sb3\n",
    "from stable_baselines3 import SAC, PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "\n",
    "import gym.spaces as gym_spaces\n",
    "import gymnasium as gym # overwrite OpenAI gym\n",
    "from gymnasium import spaces\n",
    "from gymnasium.spaces import Box\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from gym_donkeycar.envs.donkey_env import DonkeyEnv\n",
    "from stable_baselines3.common import env_checker\n",
    "\n",
    "###\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7cd6cd",
   "metadata": {},
   "source": [
    "## Load Hyperparameters from YAML config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee8b1846",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'device': device(type='cuda', index=0), 'A': 3, 'Z': 1024, 'debug': False, 'logdir': 'logs/', 'seed': 0, 'size': [128, 128], 'grayscale': True, 'toy_env': True, 'n_episodes': 5000, 'max_episode_steps': 100, 'env_id': 'donkey-minimonaco-track-v0', 'max_grad_norm': 100, 'batch_size': 8, 'H': 512, 'num_categoricals': 32, 'num_classes': 32, 'mlp_n_layers': 3, 'mlp_hidden_dims': 256, 'action_clip': 1}\n"
     ]
    }
   ],
   "source": [
    "config = load_config()\n",
    "\n",
    "for key in config:\n",
    "    locals()[key] = config[key]\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239558a5",
   "metadata": {},
   "source": [
    "## Init Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cda9f099",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing encoder:\n",
      "- adding ConvBlock((1, 32))                   ==> output shape: (32, 64, 64) ==> prod: 131072\n",
      "- adding ConvBlock((32, 64))                   ==> output shape: (64, 32, 32) ==> prod: 65536\n",
      "- adding ConvBlock((64, 128))                   ==> output shape: (128, 16, 16) ==> prod: 32768\n",
      "- adding ConvBlock((128, 256))                   ==> output shape: (256, 8, 8) ==> prod: 16384\n",
      "- adding ConvBlock((256, 64))                   ==> output shape: (64, 4, 4) ==> prod: 1024\n",
      "- adding Flatten()\n",
      "- adding Reshape: (*,1024) => (*,32,32)\n",
      "\n",
      "Initializing decoder:\n",
      "- adding Reshape: (*,1024) => (*,64,4,4)\n",
      "- adding transpose ConvBlock(64, 64)                   ==> output shape: (64, 8, 8) ==> prod: 4096\n",
      "- adding transpose ConvBlock(64, 256)                   ==> output shape: (256, 16, 16) ==> prod: 65536\n",
      "- adding transpose ConvBlock(256, 128)                   ==> output shape: (128, 32, 32) ==> prod: 131072\n",
      "- adding transpose ConvBlock(128, 64)                   ==> output shape: (64, 64, 64) ==> prod: 262144\n",
      "- adding transpose ConvBlock(64, 1)                   ==> output shape: (1, 128, 128) ==> prod: 16384\n"
     ]
    }
   ],
   "source": [
    "rssm = RSSM().to(device)\n",
    "\n",
    "rssm_optim = optim.Adam(\n",
    "    rssm.parameters(), \n",
    "    lr=1e-4,\n",
    "\n",
    "    # l2 regularizer\n",
    "    weight_decay=1e-6, \n",
    ")\n",
    "\n",
    "# value_net = MLP(input_dims=Z, output_dims=1).to(device) # state (H+Z) -> 1\n",
    "policy_net = MLP(input_dims=Z, output_dims=A, out_type=\"gaussian\").to(device) # state (H+Z) -> A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37cc84fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ContinuousActorCritic(\n",
    "     n_features=Z, \n",
    "     n_actions=A,\n",
    "     n_envs=1,\n",
    "     gamma=0.999,\n",
    "     lam=0.95,\n",
    "     entropy_coeff=0.01,\n",
    "     critic_lr=5e-4, # it's very sensitive to higher learning rates (gets nans)\n",
    "     actor_lr=1e-4,\n",
    "    action_clip=2\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4666cd2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "856cb39a",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "\n",
    "Notes:\n",
    "- currently taking random actions (not the output of the actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd076d3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\" training loop \"\"\"\n",
    "\n",
    "rssm.train()\n",
    "\n",
    "# Create the environment\n",
    "if toy_env:\n",
    "    assert A==3\n",
    "    env = gym.make(\"CarRacing-v2\", max_episode_steps=100, render_mode=\"rgb_array\") # rgb_array/human # 50 steps\n",
    "else:\n",
    "    assert A==2\n",
    "    sim_config = {\n",
    "        \"exe_path\" : \"/home/till/Desktop/Thesis/donkeycar_sim/DonkeySimLinux/donkey_sim.x86_64\",\n",
    "        \"port\" : 9091\n",
    "    }\n",
    "    env = gym.make(\n",
    "        \"GymV21Environment-v0\", \n",
    "        env_id=env_id,\n",
    "        max_episode_steps=max_episode_steps,\n",
    "        make_kwargs={\n",
    "            \"conf\": sim_config\n",
    "        })\n",
    "\n",
    "# Logging\n",
    "log_dir = \"logs/\"\n",
    "writer = SummaryWriter(log_dir)\n",
    "notebook.start(f\"--logdir={log_dir}\")\n",
    "\n",
    "episode_losses = { # for loss plots\n",
    "    \"episode_loss\": [],\n",
    "    \"episode_image_loss\": [],\n",
    "    \"episode_reward_loss\": [],\n",
    "    \"episode_continue_loss\": [],\n",
    "    \"episode_dyn_loss\": [],\n",
    "    \"episode_rep_loss\": [],\n",
    "}\n",
    "\n",
    "try:\n",
    "    for episode in tqdm(range(n_episodes)):\n",
    "\n",
    "        # Get the initial state\n",
    "        obs, info = env.reset()\n",
    "\n",
    "        # Reset the RNN's hidden state\n",
    "        h = torch.zeros(rssm.num_rnn_layers, 1, H, device=device) # seq_len, B, H\n",
    "\n",
    "        # Add a new loss for the current episode and initialize it to 0\n",
    "        episode_length = 0\n",
    "        for key in episode_losses:\n",
    "            episode_losses[key].append(torch.tensor(0, device=device, dtype=torch.float32))\n",
    "\n",
    "        # Play one episode\n",
    "        done = False\n",
    "        while not done:\n",
    "\n",
    "            x = transform(obs).view(-1, 1, 128, 128)\n",
    "\n",
    "            \"\"\" WORLD MODEL LEARNING \"\"\"\n",
    "\n",
    "            # predict z and generate the true stochastic latent variable z with the encoder\n",
    "            z_prior = rssm.dynamics_mlp(h).view(-1, num_categoricals, num_classes) # (1,32,32) for the softmax\n",
    "            z_prior = F.softmax(z_prior, -1).flatten(start_dim=1, end_dim=2) # (1, 1024)\n",
    "            z = rssm.vae.encode(x).flatten(start_dim=1, end_dim=2)\n",
    "\n",
    "            # apply external actor and critic nets on z\n",
    "            action_mean, action_var = policy_net(z)\n",
    "            action = torch.normal(mean=action_mean, std=torch.sqrt(action_var)) # # Ax1 vector\n",
    "            action = torch.clip(action, action_clip_min, action_clip_max)\n",
    "\n",
    "            v = value_net(z)\n",
    "\n",
    "            # predict one step using the RSSM\n",
    "            h, reward_pred, continue_prob, continue_pred, x_pred = rssm.step(action, h, z)\n",
    "\n",
    "            # choose and execute an action\n",
    "            next_obs, reward, terminated, truncated, info = env.step(to_np(action.squeeze()))        \n",
    "\n",
    "            done = terminated or truncated\n",
    "            obs = next_obs\n",
    "\n",
    "            # calculate the loss\n",
    "            continue_target = torch.tensor(1 - done, device=device, dtype=torch.float32)\n",
    "            reward = torch.tensor(reward, device=device, dtype=torch.float32)\n",
    "            losses = rssm.get_losses(x, x_pred, reward, reward_pred, \n",
    "                                     continue_target, continue_prob, z_prior, z)\n",
    "\n",
    "            # Add loss for the current step to the episode loss\n",
    "            episode_length += 1\n",
    "            for key in losses:\n",
    "                episode_losses[\"episode_\" + key][-1] += losses[key]\n",
    "\n",
    "        # Calculate the mean loss of the episode\n",
    "        for key in episode_losses:\n",
    "            episode_losses[key][-1] /= episode_length\n",
    "\n",
    "        # update the world model at the end of an episode using the mean loss of the episode\n",
    "        rssm_optim.zero_grad()\n",
    "        episode_losses[\"episode_loss\"][-1].backward()\n",
    "        nn.utils.clip_grad_norm_(rssm.vae.parameters(), max_norm=100.0, norm_type=2)  \n",
    "        rssm_optim.step()\n",
    "\n",
    "        # Detach the losses to save memory and log them in TensorBoard\n",
    "        for key in episode_losses:\n",
    "            episode_losses[key][-1] = episode_losses[key][-1].detach().item()\n",
    "            writer.add_scalar(key, episode_losses[key][-1], global_step=episode)\n",
    "        \n",
    "        # save original image and reconstruction\n",
    "        if episode % 10 == 0:\n",
    "            save_image_and_reconstruction(x, x_pred, episode)\n",
    "\n",
    "    env.close()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    \"\"\" Clean handling for interrupts to stop training early \"\"\"\n",
    "    print(\"Stopping training.\")\n",
    "    # Delete the last loss if the training was stopped early\n",
    "    # so that the list only consists of floats\n",
    "    for key in episode_losses:\n",
    "        if isinstance(episode_losses[key][-1], torch.Tensor):\n",
    "            episode_losses[key] = episode_losses[key][:-1]\n",
    "\n",
    "    # Close the TensorBoard writer and the gym environment\n",
    "    writer.close()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f755dc2",
   "metadata": {},
   "source": [
    "## Imagine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01a8b04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" imagine n steps \"\"\"\n",
    "\n",
    "def imagine_n_steps(obs, n, save_images=False):\n",
    "\n",
    "    if save_images:\n",
    "        images = []\n",
    "\n",
    "    # reset h\n",
    "    h = torch.zeros(rssm.num_rnn_layers, 1, H, device=device)\n",
    "\n",
    "    # encode the first state\n",
    "    x = transform(obs).view(-1, 1, 128, 128)\n",
    "    z = rssm.vae.encode(x).flatten(start_dim=1, end_dim=2)\n",
    "\n",
    "\n",
    "    for imagination_step in range(50):\n",
    "\n",
    "        # predict z from h\n",
    "        z_prior = rssm.dynamics_mlp(h).view(-1, num_categoricals, num_classes) # (1,32,32) for the softmax\n",
    "        z_prior = F.softmax(z_prior, -1).flatten(start_dim=1, end_dim=2) # (1, 1024)\n",
    "        z = z_prior\n",
    "\n",
    "        # sample an action\n",
    "        # action_mean, action_var = policy_net(z)\n",
    "        # action = torch.normal(mean=action_mean, std=torch.sqrt(action_var)) # Ax1 vector\n",
    "        # action = torch.clip(action, action_clip_min, action_clip_max)\n",
    "        \n",
    "        # predict the value\n",
    "        # v = value_net(z)\n",
    "        # action = torch.randn(2, device=device)\n",
    "        \n",
    "        action, log_prob, actor_entropy = agent.get_action(z)\n",
    "\n",
    "        # predict one step using the RSSM\n",
    "        h, reward_pred, continue_prob, continue_pred, x_pred = rssm.step(action, h, z)\n",
    "        done = 1 - continue_pred\n",
    "\n",
    "        if save_images:\n",
    "            images.append((255 * to_np(x_pred[0][0])).astype(\"uint8\"))\n",
    "\n",
    "    if save_images:\n",
    "        imageio.mimsave(\"reconstructions/imagined_episode.gif\", images, duration=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "261443c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if toy_env:\n",
    "    assert A==3\n",
    "    env = gym.make(\"CarRacing-v2\", max_episode_steps=100, render_mode=\"rgb_array\") # rgb_array/human # 50 steps\n",
    "else:\n",
    "    assert A==2\n",
    "    sim_config = {\n",
    "        \"exe_path\" : \"/home/till/Desktop/Thesis/donkeycar_sim/DonkeySimLinux/donkey_sim.x86_64\",\n",
    "        \"port\" : 9091\n",
    "    }\n",
    "    env = gym.make(\n",
    "        \"GymV21Environment-v0\", \n",
    "        env_id=env_id,\n",
    "        max_episode_steps=max_episode_steps,\n",
    "        make_kwargs={\n",
    "            \"conf\": sim_config\n",
    "        })\n",
    "obs, info = env.reset()\n",
    "\n",
    "imagine_n_steps(obs, 100, save_images=False)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5cdf3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70272184",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImaginationEnv(gym.Env):\n",
    "    \"\"\" Custom gymnasium environment for training inside the world model (RSSM). \"\"\"\n",
    "    def __init__(self):\n",
    "        super(ImaginationEnv, self).__init__()\n",
    "        \n",
    "        # define action and observation space\n",
    "        # they must be gym.spaces objects\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(H+Z,), dtype=np.float32)\n",
    "        self.action_space = spaces.Box(low=-action_clip, high=action_clip, shape=(A,), dtype=np.float32)\n",
    "    \n",
    "    def step(self, action):\n",
    "        observation = np.random.rand(H+Z)\n",
    "        reward = np.random.rand(1).item()\n",
    "        terminated = np.random.rand(1).item() > 0.5\n",
    "        truncated = np.random.rand(1).item() > 0.5\n",
    "        info = {}\n",
    "        return observation, reward, terminated, truncated, info\n",
    "    \n",
    "    def reset(self):\n",
    "        observation = np.random.rand(H+Z)\n",
    "        info = {}\n",
    "        return observation, info\n",
    "\n",
    "    def render(self):\n",
    "        pass\n",
    "    \n",
    "    def close(self):\n",
    "        pass\n",
    "    \n",
    "imagination_env = ImaginationEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "751b07ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.40952018, 0.5596945 , 0.55404613, ..., 0.31293196, 0.46543275,\n",
       "        0.97797887]),\n",
       " {})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imagination_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "419c520d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.88832871, 0.31962352, 0.60064864, ..., 0.36925936, 0.64272986,\n",
       "        0.4995378 ]),\n",
       " 0.29841965459690645,\n",
       " False,\n",
       " True,\n",
       " {})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imagination_env.step(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9277854c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "agent = PPO(\"MlpPolicy\", imagination_env, verbose=1, tensorboard_log=\"logs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7f8b2bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0a5'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sb3.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12392a20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92a67a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343a31fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6078919b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844ca9b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e216ab1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccb6917",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab13016b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503095f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb5ae91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97966f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d803da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55e74d14",
   "metadata": {},
   "source": [
    "## Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0703ff02",
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_length = max(1, int(len(episode_losses[\"episode_loss\"])/20))\n",
    "\n",
    "fig, axs = plt.subplots(ncols=3, nrows=2, figsize=(3*5, 2*5))\n",
    "\n",
    "# Iterate over the keys and plot the losses\n",
    "for i, key in enumerate(episode_losses.keys()):\n",
    "    row = i // 3\n",
    "    col = i % 3\n",
    "\n",
    "    axs[row, col].set_title(key)\n",
    "    losses = episode_losses[key]\n",
    "    losses_moving_average = (\n",
    "        np.convolve(\n",
    "            np.array(losses).flatten(), np.ones(rolling_length), mode=\"valid\"\n",
    "        )\n",
    "        / rolling_length\n",
    "    )\n",
    "    axs[row, col].plot(range(len(losses)), losses, label=key)\n",
    "    axs[row, col].plot(range(len(losses_moving_average)), losses_moving_average, label=\"moving average\")\n",
    "    axs[row, col].legend(loc=\"upper right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3354e868",
   "metadata": {},
   "source": [
    "## Test area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f6ffa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU memory consumption:\n",
    "# 10 steps -> 6504MiB\n",
    "# 50 steps -> 19990MiB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71f928a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0125831e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" save reconstructions for one episode \"\"\"\n",
    "\n",
    "save_one_episode = False\n",
    "\n",
    "\n",
    "if save_one_episode:\n",
    "    rssm.eval()\n",
    "\n",
    "    # Create the environment\n",
    "    if toy_env:\n",
    "        assert A==3\n",
    "        env = gym.make(\"CarRacing-v2\", max_episode_steps=150, render_mode=\"rgb_array\") # rgb_array/human # 50 steps\n",
    "    else:\n",
    "        assert A==2\n",
    "        sim_config = {\n",
    "            \"exe_path\" : \"/home/till/Desktop/Thesis/donkeycar_sim/DonkeySimLinux/donkey_sim.x86_64\",\n",
    "            \"port\" : 9091\n",
    "        }\n",
    "        env = gym.make(\n",
    "            \"GymV21Environment-v0\", \n",
    "            env_id=env_id,\n",
    "            max_episode_steps=max_episode_steps,\n",
    "            make_kwargs={\n",
    "                \"conf\": sim_config\n",
    "            })\n",
    "\n",
    "\n",
    "    try:\n",
    "        for episode in tqdm(range(1)):\n",
    "\n",
    "            # Get the initial state\n",
    "            obs, info = env.reset()\n",
    "\n",
    "            # Reset the RNN's hidden state\n",
    "            h = torch.zeros(rssm.num_rnn_layers, 1, H, device=device) # seq_len, B, H\n",
    "\n",
    "            # Add a new loss for the current episode and initialize it to 0\n",
    "            episode_length = 0\n",
    "\n",
    "            # Play one episode\n",
    "            done = False\n",
    "            while not done:\n",
    "\n",
    "                x = transform(obs).view(-1, 1, 128, 128)\n",
    "\n",
    "                \"\"\" WORLD MODEL LEARNING \"\"\"\n",
    "\n",
    "                # predict z and generate the true stochastic latent variable z with the encoder\n",
    "                z_prior = rssm.dynamics_mlp(h).view(-1, num_categoricals, num_classes) # (1,32,32) for the softmax\n",
    "                z_prior = F.softmax(z_prior, -1).flatten(start_dim=1, end_dim=2) # (1, 1024)\n",
    "                z = rssm.vae.encode(x).flatten(start_dim=1, end_dim=2)\n",
    "\n",
    "                # apply external actor and critic nets on z\n",
    "                action = torch.tensor([(np.random.rand() - 0.5)/3, (np.random.rand() + 0.5)/3], device=device).unsqueeze(dim=0) # policy_net(z) # Ax1 vector\n",
    "                v = value_net(z)\n",
    "\n",
    "                # predict one step using the RSSM and apply the actor-critic\n",
    "                h, reward_pred, continue_prob, continue_pred, x_pred = rssm.step(action, h, z)\n",
    "\n",
    "                # choose and execute an action\n",
    "                next_obs, reward, terminated, truncated, info = env.step(to_np(action.squeeze()))        \n",
    "\n",
    "                done = terminated or truncated\n",
    "                obs = next_obs\n",
    "\n",
    "                # calculate the loss\n",
    "                continue_target = torch.tensor(1 - done, device=device, dtype=torch.float32)\n",
    "                reward = torch.tensor(reward, device=device, dtype=torch.float32)\n",
    "\n",
    "                # TODO: z_prior, z_posterior\n",
    "                z_prior = torch.tensor(0, device=device, dtype=torch.float32)\n",
    "                z_posterior = torch.tensor(0, device=device, dtype=torch.float32)\n",
    "\n",
    "                plt.imsave(f\"reconstructions/{episode_length}.png\", to_np(x_pred[0][0]), cmap=\"gray\")\n",
    "                episode_length += 1\n",
    "\n",
    "        env.close()\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        \"\"\" Clean handling for interrupts to stop training early \"\"\"\n",
    "        print(\"Stopping training.\")\n",
    "        # Delete the last loss if the training was stopped early\n",
    "        # so that the list only consists of floats\n",
    "        for key in episode_losses:\n",
    "            if isinstance(episode_losses[key][-1], torch.Tensor):\n",
    "                episode_losses[key] = episode_losses[key][:-1]\n",
    "\n",
    "        # Close the TensorBoard writer and the gym environment\n",
    "        writer.close()\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6952d981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.11.0 at http://localhost:6006/ (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=\"logs/\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5c58c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
