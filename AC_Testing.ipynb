{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43db25de",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d80a4c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from collections import deque\n",
    "from operator import itemgetter\n",
    "\n",
    "import gym_donkeycar\n",
    "import gymnasium as gym\n",
    "import imageio\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from IPython.display import display\n",
    "from ipywidgets import HBox, VBox\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from ruamel.yaml import YAML\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.stats import norm\n",
    "from tensorboard import notebook\n",
    "from tensorboard.backend.event_processing.event_accumulator import \\\n",
    "    EventAccumulator\n",
    "from torch import distributions as dist\n",
    "from torch.distributions import Categorical, Normal\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gym.spaces as gym_spaces\n",
    "import gymnasium as gym  # overwrite OpenAI gym\n",
    "\n",
    "# suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"gymnasium.spaces.box\") # module=\"gymnasium\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ[\"IMAGEIO_IGNORE_WARNINGS\"] = \"True\"\n",
    "\n",
    "import stable_baselines3 as sb3\n",
    "from gym_donkeycar.envs.donkey_env import DonkeyEnv\n",
    "from gymnasium import spaces\n",
    "from gymnasium.spaces import Box\n",
    "from stable_baselines3 import A2C, PPO, SAC\n",
    "from stable_baselines3.common import env_checker\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "\n",
    "from src.actor_critic_discrete import DiscreteActorCritic\n",
    "from src.actor_critic_dreamer import ActorCriticDreamer\n",
    "from src.actor_critic import ContinuousActorCritic\n",
    "from src.blocks import CategoricalStraightThrough, ConvBlock\n",
    "from src.categorical_vae import CategoricalVAE\n",
    "from src.imagination_env import make_imagination_env\n",
    "from src.mlp import MLP\n",
    "from src.preprocessing import transform\n",
    "from src.replay_buffer import ReplayBuffer\n",
    "from src.rssm import RSSM\n",
    "from src.utils import (load_config, make_env, save_image_and_reconstruction,\n",
    "                       to_np, symlog, symexp, twohot_encode, ExponentialMovingAvg,\n",
    "                       ActionExponentialMovingAvg)\n",
    "from src.vae import VAE\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Load the config\n",
    "config = load_config()\n",
    "for key in config:\n",
    "    locals()[key] = config[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1daa9ad2",
   "metadata": {},
   "source": [
    "## Create the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4d6530b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Pendulum-v1\")\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env, deque_size=config[\"n_updates\"])\n",
    "env = gym.wrappers.TimeLimit(env, max_episode_steps=config[\"max_episode_steps\"])\n",
    "env = gym.wrappers.AutoResetWrapper(env)\n",
    "env = gym.experimental.wrappers.RescaleActionV0(env, min_action=config[\"action_space_low\"], max_action=config[\"action_space_high\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2917ae7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "15e3503e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding zero weight init to the output layer.\n"
     ]
    }
   ],
   "source": [
    "# agent = ContinuousActorCritic()\n",
    "# agent = ActorCriticDreamer()\n",
    "agent = DiscreteActorCritic()\n",
    "\n",
    "# agent.load_weights(\"weights/ContinuousActorCritic_0\")\n",
    "\n",
    "# vae = VAE()\n",
    "# vae.optim = optim.Adam(vae.parameters(), lr=1e-4, weight_decay=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "dad6541a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|██████████████▏                                                                   | 86578/500000 [51:33<4:06:12, 27.99it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [127], line 37\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_steps_per_update):\n\u001b[1;32m     34\u001b[0m         \n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# Add the observation, reconstruction, mu, and logvar to the respective batches\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     batch_observations\u001b[38;5;241m.\u001b[39mappend(obs)\n\u001b[0;32m---> 37\u001b[0m     value_pred, critic_dist \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_critic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# obs for Pendulum | z for VAE\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# Get an action and take an environment step\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     action, log_prob, actor_entropy \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mget_action(obs) \u001b[38;5;66;03m### obs for Pendulum | z for VAE\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/GitHub/self-driving-car/src/actor_critic_discrete.py:93\u001b[0m, in \u001b[0;36mDiscreteActorCritic.apply_critic\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     90\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(x)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     92\u001b[0m buckets \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_bucket, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_bucket, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_buckets)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 93\u001b[0m value_pred \u001b[38;5;241m=\u001b[39m \u001b[43msymexp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcritic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbuckets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m critic_dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic(x)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m value_pred, critic_dist\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# New training loop with batches for the distributional critic\n",
    "\n",
    "losses = {\n",
    "    \"critic_loss\": [],\n",
    "    \"actor_loss\": [],\n",
    "}\n",
    "\n",
    "# Logging\n",
    "writer = SummaryWriter(log_dir)\n",
    "if config[\"show_inline_tensorboard\"]:\n",
    "    notebook.start(f\"--logdir={log_dir}\")\n",
    "\n",
    "for sample_phase in tqdm(range(n_updates)):\n",
    "    \n",
    "    batch_observations = []\n",
    "    \n",
    "    # NEW\n",
    "    batch_rewards = []\n",
    "    batch_log_probs = []\n",
    "    batch_value_preds = []\n",
    "    batch_entropies = []\n",
    "    batch_masks = []\n",
    "    batch_critic_dists = []\n",
    "    \n",
    "    if sample_phase == 0:\n",
    "        obs, info = env.reset(seed=42)\n",
    "        obs = torch.Tensor(obs).to(device) ### Pendulum\n",
    "        # obs = transform(obs)\n",
    "        # if len(obs.shape) == 4:\n",
    "        #     obs = obs[0]\n",
    "        # z, reconstruction, mu, logvar = vae(obs)\n",
    "\n",
    "    for step in range(n_steps_per_update):\n",
    "            \n",
    "        # Add the observation, reconstruction, mu, and logvar to the respective batches\n",
    "        batch_observations.append(obs)\n",
    "        value_pred, critic_dist = agent.apply_critic(obs) # obs for Pendulum | z for VAE\n",
    "\n",
    "        # Get an action and take an environment step\n",
    "        action, log_prob, actor_entropy = agent.get_action(obs) ### obs for Pendulum | z for VAE\n",
    "        obs, reward, terminated, truncated, info = env.step(to_np(action))\n",
    "        \n",
    "        # Transform the next obs\n",
    "        obs = torch.Tensor(obs).to(device) ### Pendulum\n",
    "        # obs = transform(obs)\n",
    "        # if len(obs.shape) == 4:\n",
    "        #     obs = obs[0]\n",
    "        # z, reconstruction, mu, logvar = vae(obs)\n",
    "        \n",
    "        # Collect the necessary data for an agent update\n",
    "        batch_rewards.append(reward)\n",
    "        batch_log_probs.append(log_prob)\n",
    "        batch_entropies.append(actor_entropy)\n",
    "        mask = torch.tensor(0.0 if terminated else 1.0)\n",
    "        batch_masks.append(mask)\n",
    "        batch_value_preds.append(value_pred)\n",
    "        batch_critic_dists.append(critic_dist)\n",
    "\n",
    "    # Convert the batch tensors to tensors\n",
    "    batch_observations = torch.stack(batch_observations).to(device)  # [n_steps_per_update, *obs_shape]\n",
    "    batch_rewards = torch.tensor(batch_rewards).to(device)  # [n_steps_per_update]\n",
    "    batch_log_probs = torch.stack(batch_log_probs).to(device)  # [n_steps_per_update]\n",
    "    batch_value_preds = torch.stack(batch_value_preds).to(device)  # [n_steps_per_update]\n",
    "    batch_entropies = torch.stack(batch_entropies).to(device)  # [n_steps_per_update]\n",
    "    batch_masks = torch.stack(batch_masks).to(device)  # [n_steps_per_update]\n",
    "    batch_critic_dists = torch.stack(batch_critic_dists).to(device)\n",
    "    last_value_pred, _ = agent.apply_critic(obs) # last value prediction for GAE\n",
    "\n",
    "    # Update the agent's parameters\n",
    "    critic_loss, actor_loss = agent.get_loss(\n",
    "        batch_rewards, batch_log_probs, batch_value_preds, batch_critic_dists, \n",
    "        last_value_pred, batch_entropies, batch_masks\n",
    "    )\n",
    "    agent.update_parameters(critic_loss, actor_loss)\n",
    "    \n",
    "    # Update the VAE's parameters\n",
    "    # vae_loss, reconstruction_loss, kld_loss = vae.get_loss(batch_, x_hat, mu, logvar)\n",
    "    # vae.optim.zero_grad()\n",
    "    # vae_loss.backward()\n",
    "    # vae.optim.step()\n",
    "\n",
    "    if sample_phase % config[\"log_interval\"] == 0:\n",
    "        \n",
    "        # Log the losses\n",
    "        losses[\"critic_loss\"].append(to_np(critic_loss))\n",
    "        losses[\"actor_loss\"].append(to_np(actor_loss))\n",
    "\n",
    "        # Log the episode metrics\n",
    "        if len(env.return_queue):\n",
    "            writer.add_scalar(\"episode_return\", np.array(env.return_queue)[-1:], global_step=len(env.return_queue))\n",
    "        writer.add_scalar(\"critic_loss\", to_np(critic_loss), global_step=sample_phase)\n",
    "        writer.add_scalar(\"actor_loss\", to_np(actor_loss), global_step=sample_phase)\n",
    "        writer.add_scalar(\"scaled_entropy_loss\", to_np(- agent.ent_coef * torch.sum(batch_entropies)), global_step=sample_phase)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0640db5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c656127b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5751c7b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcf250f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefcc45f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
