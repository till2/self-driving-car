{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43db25de",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d80a4c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from collections import deque\n",
    "from operator import itemgetter\n",
    "\n",
    "import gym_donkeycar\n",
    "import gymnasium as gym\n",
    "import imageio\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from IPython.display import display\n",
    "from ipywidgets import HBox, VBox\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from ruamel.yaml import YAML\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.stats import norm\n",
    "from tensorboard import notebook\n",
    "from tensorboard.backend.event_processing.event_accumulator import \\\n",
    "    EventAccumulator\n",
    "from torch import distributions as dist\n",
    "from torch.distributions import Categorical, Normal\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "# suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"gymnasium.spaces.box\") # module=\"gymnasium\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ[\"IMAGEIO_IGNORE_WARNINGS\"] = \"True\"\n",
    "\n",
    "import stable_baselines3 as sb3\n",
    "from gym_donkeycar.envs.donkey_env import DonkeyEnv\n",
    "from gymnasium import spaces\n",
    "from gymnasium.spaces import Box\n",
    "from stable_baselines3 import A2C, PPO, SAC\n",
    "from stable_baselines3.common import env_checker\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "\n",
    "import src\n",
    "from src.actor_critic_discrete import DiscreteActorCritic\n",
    "from src.actor_critic import ContinuousActorCritic\n",
    "from src.blocks import CategoricalStraightThrough, ConvBlock\n",
    "from src.categorical_vae import CategoricalVAE\n",
    "from src.imagination_env import make_imagination_env\n",
    "from src.mlp import MLP\n",
    "from src.preprocessing import transform\n",
    "from src.replay_buffer import ReplayBuffer\n",
    "from src.rssm import RSSM\n",
    "from src.utils import (load_config, make_env, save_image_and_reconstruction,\n",
    "                       to_np, symlog, symexp, twohot_encode, ExponentialMovingAvg,\n",
    "                       ActionExponentialMovingAvg, MetricsTracker)\n",
    "from src.vae import VAE\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Load the config\n",
    "config = load_config()\n",
    "for key in config:\n",
    "    locals()[key] = config[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1daa9ad2",
   "metadata": {},
   "source": [
    "## Create the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "a4d6530b",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_one_env = lambda: gym.make(\"Pendulum-v1\")\n",
    "make_time_limit_env = lambda: gym.wrappers.TimeLimit(make_one_env(), max_episode_steps=config[\"max_episode_steps\"])\n",
    "make_auto_reset_env = lambda: gym.wrappers.AutoResetWrapper(make_time_limit_env())\n",
    "\n",
    "env = gym.vector.AsyncVectorEnv([lambda: make_auto_reset_env() for i in range(n_envs)])\n",
    "\n",
    "env = gym.experimental.wrappers.RescaleActionV0(env, min_action=config[\"action_space_low\"], max_action=config[\"action_space_high\"])\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env, deque_size=config[\"n_updates\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "969fdb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = make_env()\n",
    "# \n",
    "# env.reset()\n",
    "# for i in range(1000):\n",
    "#     obs, reward, terminated, truncated, info = env.step(np.random.rand(5,3)*100)\n",
    "#     done = [te or tr for te, tr in zip(terminated, truncated)]\n",
    "#     print(done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "15e3503e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing agent with 3 features and 1 actions.\n",
      "Initializing critic.\n",
      "Adding zero weight init to the output layer.\n",
      "Initializing actor.\n",
      "Adding zero weight init to the output layer.\n"
     ]
    }
   ],
   "source": [
    "# agent = ContinuousActorCritic()\n",
    "agent = DiscreteActorCritic(n_features=3, n_actions=1)\n",
    "\n",
    "# agent.load_weights(\"weights/ContinuousActorCritic_0\")\n",
    "\n",
    "# vae = VAE()\n",
    "# vae.optim = optim.Adam(vae.parameters(), lr=1e-4, weight_decay=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "d8f90f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|████████████▌                                                                     | 76888/500000 [50:28<4:37:46, 25.39it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [272], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m last_value_pred, _ \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mapply_critic(obs) \u001b[38;5;66;03m# last value prediction for GAE\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Update the agent's parameters\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m critic_loss, actor_loss \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepisode_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_value_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m agent\u001b[38;5;241m.\u001b[39mupdate_parameters(critic_loss, actor_loss)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Log the episode metrics\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/GitHub/self-driving-car/src/actor_critic_discrete.py:183\u001b[0m, in \u001b[0;36mDiscreteActorCritic.get_loss\u001b[0;34m(self, episode_batches, last_value_pred)\u001b[0m\n\u001b[1;32m    180\u001b[0m     advantages[t] \u001b[38;5;241m=\u001b[39m next_advantage \u001b[38;5;241m=\u001b[39m td_error \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlam \u001b[38;5;241m*\u001b[39m ep_masks[t] \u001b[38;5;241m*\u001b[39m next_advantage\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m# categorical crossentropy (should be fine, I checked.)\u001b[39;00m\n\u001b[0;32m--> 183\u001b[0m twohot_returns \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([twohot_encode(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m returns]) \u001b[38;5;66;03m# (SEQ_LEN, B, NUM_BUCKETS)        \u001b[39;00m\n\u001b[1;32m    184\u001b[0m critic_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calculate_critic_loss(twohot_returns, batch_critic_dists)\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# calculate the actor loss using the policy gradient theorem and give an entropy bonus\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/GitHub/self-driving-car/src/actor_critic_discrete.py:183\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    180\u001b[0m     advantages[t] \u001b[38;5;241m=\u001b[39m next_advantage \u001b[38;5;241m=\u001b[39m td_error \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlam \u001b[38;5;241m*\u001b[39m ep_masks[t] \u001b[38;5;241m*\u001b[39m next_advantage\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m# categorical crossentropy (should be fine, I checked.)\u001b[39;00m\n\u001b[0;32m--> 183\u001b[0m twohot_returns \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\u001b[43mtwohot_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m returns]) \u001b[38;5;66;03m# (SEQ_LEN, B, NUM_BUCKETS)        \u001b[39;00m\n\u001b[1;32m    184\u001b[0m critic_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calculate_critic_loss(twohot_returns, batch_critic_dists)\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# calculate the actor loss using the policy gradient theorem and give an entropy bonus\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/GitHub/self-driving-car/src/utils.py:52\u001b[0m, in \u001b[0;36mtwohot_encode\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtwohot_encode\u001b[39m(x):\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    Computes the twohot encoded symlog returns.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m        - note that the returns are automatically symlogged before the two-hot encoding\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[43mload_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m     55\u001b[0m         x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(x)\n",
      "File \u001b[0;32m~/Desktop/GitHub/self-driving-car/src/utils.py:95\u001b[0m, in \u001b[0;36mload_config\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(parent_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msrc/config.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m---> 95\u001b[0m     static_params \u001b[38;5;241m=\u001b[39m \u001b[43myaml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# Set dependent variables\u001b[39;00m\n\u001b[1;32m     98\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;129;01mand\u001b[39;00m static_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ruamel/yaml/main.py:434\u001b[0m, in \u001b[0;36mYAML.load\u001b[0;34m(self, stream)\u001b[0m\n\u001b[1;32m    432\u001b[0m constructor, parser \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_constructor_parser(stream)\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 434\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconstructor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_single_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    436\u001b[0m     parser\u001b[38;5;241m.\u001b[39mdispose()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ruamel/yaml/constructor.py:119\u001b[0m, in \u001b[0;36mBaseConstructor.get_single_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_single_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# type: () -> Any\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m# Ensure that the stream contains a single document and construct it.\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomposer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_single_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m node \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconstruct_document(node)\n",
      "File \u001b[0;32m_ruamel_yaml.pyx:706\u001b[0m, in \u001b[0;36m_ruamel_yaml.CParser.get_single_node\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_ruamel_yaml.pyx:724\u001b[0m, in \u001b[0;36m_ruamel_yaml.CParser._compose_document\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_ruamel_yaml.pyx:775\u001b[0m, in \u001b[0;36m_ruamel_yaml.CParser._compose_node\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_ruamel_yaml.pyx:888\u001b[0m, in \u001b[0;36m_ruamel_yaml.CParser._compose_mapping_node\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_ruamel_yaml.pyx:771\u001b[0m, in \u001b[0;36m_ruamel_yaml.CParser._compose_node\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_ruamel_yaml.pyx:801\u001b[0m, in \u001b[0;36m_ruamel_yaml.CParser._compose_scalar_node\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ruamel/yaml/resolver.py:370\u001b[0m, in \u001b[0;36mVersionedResolver.resolve\u001b[0;34m(self, kind, value, implicit)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m     resolvers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mversioned_resolver\u001b[38;5;241m.\u001b[39mget(value[\u001b[38;5;241m0\u001b[39m], [])\n\u001b[0;32m--> 370\u001b[0m resolvers \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mversioned_resolver\u001b[49m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28;01mNone\u001b[39;00m, [])\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tag, regexp \u001b[38;5;129;01min\u001b[39;00m resolvers:\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m regexp\u001b[38;5;241m.\u001b[39mmatch(value):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ruamel/yaml/resolver.py:357\u001b[0m, in \u001b[0;36mVersionedResolver.versioned_resolver\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(version, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    356\u001b[0m     version \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mint\u001b[39m, version\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[0;32m--> 357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_version_implicit_resolver:\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m implicit_resolvers:\n\u001b[1;32m    359\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m version \u001b[38;5;129;01min\u001b[39;00m x[\u001b[38;5;241m0\u001b[39m]:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# New training loop with batches for the distributional critic\n",
    "\n",
    "tracker = MetricsTracker(\n",
    "    training_metrics=[\"critic_loss\", \"actor_loss\"],\n",
    "    episode_metrics=[\"rewards\", \"log_probs\", \"value_preds\", \"critic_dists\", \"entropies\", \"masks\"],\n",
    ")\n",
    "\n",
    "for sample_phase in tqdm(range(n_updates)):\n",
    "    \n",
    "    if sample_phase == 0:\n",
    "        obs, info = env.reset(seed=42)\n",
    "        # obs = transform(torch.tensor(obs)) ### only for images\n",
    "\n",
    "    for step in range(n_steps_per_update):\n",
    "            \n",
    "        value_pred, critic_dist = agent.apply_critic(obs)\n",
    "\n",
    "        # Get an action and take an environment step\n",
    "        action, log_prob, actor_entropy = agent.get_action(obs)\n",
    "        obs, reward, terminated, truncated, info = env.step(to_np(action))\n",
    "        # obs = transform(torch.tensor(obs)) ### only for images\n",
    "        \n",
    "        # every step:\n",
    "        tracker.add(\n",
    "            episode_metrics={\n",
    "                \"rewards\": reward,\n",
    "                \"log_probs\": log_prob,\n",
    "                \"value_preds\": value_pred,\n",
    "                \"critic_dists\": critic_dist,\n",
    "                \"entropies\": actor_entropy,\n",
    "                \"masks\": np.where(terminated, 0, 1),\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    # every sample phase:\n",
    "    episode_batches = tracker.get_episode_batches() # episode_batches is a dict\n",
    "    last_value_pred, _ = agent.apply_critic(obs) # last value prediction for GAE\n",
    "\n",
    "    # Update the agent's parameters\n",
    "    critic_loss, actor_loss = agent.get_loss(episode_batches, last_value_pred)\n",
    "    agent.update_parameters(critic_loss, actor_loss)\n",
    "    \n",
    "    # Log the episode metrics\n",
    "    if sample_phase % config[\"log_interval\"] == 0:\n",
    "        tracker.add(\n",
    "            training_metrics={\n",
    "                \"critic_loss\": critic_loss,\n",
    "                \"actor_loss\": actor_loss,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Episode return\n",
    "        if len(env.return_queue):\n",
    "            tracker.writer.add_scalar(\"episode_return\", np.array(env.return_queue)[-1], global_step=len(env.return_queue))\n",
    "\n",
    "        # Actor and critic losses\n",
    "        tracker.log_to_tensorboard(step=sample_phase)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d6fb66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de88d4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "db0cddbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_one_env = lambda: gym.make(\"Pendulum-v1\", render_mode=\"human\")\n",
    "make_time_limit_env = lambda: gym.wrappers.TimeLimit(make_one_env(), max_episode_steps=config[\"max_episode_steps\"])\n",
    "make_auto_reset_env = lambda: gym.wrappers.AutoResetWrapper(make_time_limit_env())\n",
    "\n",
    "env = gym.vector.AsyncVectorEnv([lambda: make_auto_reset_env() for i in range(1)])\n",
    "\n",
    "env = gym.experimental.wrappers.RescaleActionV0(env, min_action=config[\"action_space_low\"], max_action=config[\"action_space_high\"])\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env, deque_size=config[\"n_updates\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "fbc45352",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, info = env.reset()\n",
    "\n",
    "for i in range(1000):\n",
    "    action, log_prob, actor_entropy = agent.get_action(obs)\n",
    "    obs, reward, terminated, truncated, info = env.step(to_np(action))\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b753e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b58485",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
