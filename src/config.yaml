# Setup
debug: False
log_dir: "logs/"
log_interval: 5 # log every 50 steps
seed: 0
device: "cuda:0" # "cuda:0" | "cpu"

# Donkey Car Simulator
exe_path: "/home/till/Desktop/Thesis/donkeycar_sim/DonkeySimLinux/donkey_sim.x86_64"
env_id: "donkey-minimonaco-track-v0" # "donkey-generated-roads-v0"
port: 9091
max_cte: 4.0
frame_skip: 2
steer_limit: 1.0
throttle_min: -0.2 # the action clip gets applied on top of this. e.g.: action_clip=0.5 => throttle is between [-0.1, 0.1]
throttle_max: 0.2
body_style: "f1" # "donkey" | "bare" | "car01" | "f1" | "cybertruck"
body_rgb: [255, 255, 255]
car_name: "RL-Racer"
font_size: 30

# Vectorized Envs
sb3_monitor: False
toy_env: True
vectorized: True
n_envs: 1

# Observations
size: [64, 64] # [128, 128]
grayscale: False

# Training
start_episode: 1000 # if starting from a pretrained rssm (to skip the seed episodes)
n_seed_episodes: 1000
n_training_episodes: 5000
max_imagination_episode_steps: 16
imagination_timesteps_per_model_update: 200
max_grad_norm: 100

# RSSM
rssm_lr: 1e-4
rssm_l2_regularization: 1e-6
batch_size: 1
H: 512
# Z is set in the utils file
# A is set in the utils file
uniform_ratio: 0.01
buffer_size: 50_000 # for Dreamer (latent vectors); reduce the size if storing images for the vae (for vae baselines)
activation: "silu" # "silu", "elu", "relu"

# Categorical VAE
num_categoricals: 32
num_classes: 32
channels: [64, 128, 256, 512, 256] # für 2x2 bei shape [64, 64]

# [32, 64, 128, 256, 256, 256] für 2x2 bei shape [128, 128]
# [32, 64, 128, 256, 64] for 4x4 bei shape [128, 128]

kernel_size: 3
stride: 2
padding: 1
conv_bias: False
entropyloss_coeff: 0.0
decoder_final_activation: "sigmoid" # "sigmoid" | None

# RSSM
pred_loss_coeff: 1.0
dyn_loss_coeff: 0.5
rep_loss_coeff: 0.1
free_nats: 1.0
num_rnn_layers: 1

# MLP
mlp_n_layers: 3
mlp_hidden_dims: 256

# Replay Buffer
store_on_cpu: True

# RL Agent (SB3)
gamma: 0.997
lam: 0.95 # lam=1 is Monte-Carlo (no bias, high variance), lam=0 is TD (high bias, no variance)
ent_coef: 3e-4
moving_avg_decay_rate: 0.99
# actor_loss: "reinforce" # reinforce | dynamics_backprop
verbose: 0
imagination_progress_bar: True
# action_clip: 1.0 # action range: [-action_clip, +action_clip] # uses half the action space with clip=0.5 
# action_clip is now removed. change steer_limit and throttle_min/max instead.
action_space_low: -1
action_space_high: 1

# ContinuousActorCritic
critic_lr: 1e-4
actor_lr: 5e-5

# DiscreteActorCritic
action_mov_avg_decay: 0.5 # fraction of previous action that stays (higher values would lead to smoother control but slower reaction time)

# ActorCriticDreamer
num_buckets: 255
min_bucket: -15
max_bucket: 15


# Baselines Training
max_episode_steps: 1000 # 1000 steps => about 20 GB with a 20M-parameters model
n_updates: 500000 # 31250
n_steps_per_update: 16

# Gaussian VAE (Baselines)
gaussian_vae_buffer_size: 30000
vae_batch_size: 32 # for training on samples in the replay buffer
add_obs_to_buffer_interval: 10 # add an image every N steps for diversity
gaussian_vae_beta: 1.0 # KLD loss coef
gaussian_vae_lr: 1e-4
gaussian_vae_weight_decay: 1e-6