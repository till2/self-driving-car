{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b1c5dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from collections import deque\n",
    "from operator import itemgetter\n",
    "\n",
    "import gym_donkeycar\n",
    "import gymnasium as gym\n",
    "import imageio\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from IPython.display import display\n",
    "from ipywidgets import HBox, VBox\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from ruamel.yaml import YAML\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.stats import norm\n",
    "from tensorboard import notebook\n",
    "from tensorboard.backend.event_processing.event_accumulator import \\\n",
    "    EventAccumulator\n",
    "from torch import distributions as dist\n",
    "from torch.distributions import Categorical, Normal\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "# suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"gymnasium.spaces.box\") # module=\"gymnasium\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ[\"IMAGEIO_IGNORE_WARNINGS\"] = \"True\"\n",
    "\n",
    "import stable_baselines3 as sb3\n",
    "from gym_donkeycar.envs.donkey_env import DonkeyEnv\n",
    "from gymnasium import spaces\n",
    "from gymnasium.spaces import Box\n",
    "from stable_baselines3 import A2C, PPO, SAC\n",
    "from stable_baselines3.common import env_checker\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "\n",
    "import src\n",
    "from src.actor_critic_discrete import DiscreteActorCritic\n",
    "from src.actor_critic_dreamer import ActorCriticDreamer\n",
    "from src.actor_critic import ContinuousActorCritic\n",
    "from src.blocks import CategoricalStraightThrough, ConvBlock\n",
    "from src.categorical_vae import CategoricalVAE\n",
    "from src.imagination_env import make_imagination_env\n",
    "from src.mlp import MLP\n",
    "from src.preprocessing import transform\n",
    "from src.replay_buffer import ReplayBuffer\n",
    "from src.rssm import RSSM\n",
    "from src.utils import (load_config, make_env, save_image_and_reconstruction,\n",
    "                       to_np, symlog, symexp, twohot_encode, ExponentialMovingAvg,\n",
    "                       ActionExponentialMovingAvg, MetricsTracker)\n",
    "from src.vae import VAE\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Load the config\n",
    "config = load_config()\n",
    "for key in config:\n",
    "    locals()[key] = config[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1af578e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'device': device(type='cuda', index=0), 'A': 3, 'Z': 1024, 'debug': False, 'log_dir': 'logs/', 'log_interval': 20, 'seed': 0, 'exe_path': '/home/till/Desktop/Thesis/donkeycar_sim/DonkeySimLinux/donkey_sim.x86_64', 'env_id': 'donkey-minimonaco-track-v0', 'port': 9091, 'max_cte': 4.0, 'frame_skip': 2, 'steer_limit': 1.0, 'throttle_min': -0.2, 'throttle_max': 0.2, 'body_style': 'f1', 'body_rgb': [255, 255, 255], 'car_name': 'RL-Racer', 'font_size': 30, 'sb3_monitor': False, 'toy_env': True, 'vectorized': True, 'n_envs': 1, 'size': [64, 64], 'grayscale': False, 'start_phase': 1000, 'n_seed_phases': 1000, 'n_model_updates': 7812, 'n_steps_per_model_update': 64, 'agent_update_phases_per_model_update': 5, 'max_imagination_episode_steps': 16, 'imagination_steps_per_agent_update': 128, 'max_grad_norm': 100, 'rssm_lr': 0.0001, 'rssm_l2_regularization': 1e-06, 'batch_size': 1, 'H': 512, 'uniform_ratio': 0.01, 'buffer_size': 50000, 'activation': 'silu', 'num_categoricals': 32, 'num_classes': 32, 'channels': [64, 128, 256, 512, 256], 'kernel_size': 3, 'stride': 2, 'padding': 1, 'conv_bias': False, 'entropyloss_coeff': 0.0, 'decoder_final_activation': 'sigmoid', 'pred_loss_coeff': 1.0, 'dyn_loss_coeff': 0.5, 'rep_loss_coeff': 0.1, 'free_nats': 1.0, 'num_rnn_layers': 1, 'mlp_n_layers': 3, 'mlp_hidden_dims': 256, 'store_on_cpu': True, 'gamma': 0.997, 'lam': 0.95, 'ent_coef': 0.0003, 'moving_avg_decay_rate': 0.99, 'verbose': 0, 'imagination_progress_bar': True, 'action_space_low': -1, 'action_space_high': 1, 'critic_lr': 0.0001, 'actor_lr': 5e-05, 'action_mov_avg_decay': 0.5, 'num_buckets': 255, 'min_bucket': -15, 'max_bucket': 15, 'max_episode_steps': 1000, 'n_updates': 500000, 'n_steps_per_update': 16, 'gaussian_vae_buffer_size': 30000, 'vae_batch_size': 32, 'add_obs_to_buffer_interval': 10, 'gaussian_vae_beta': 1.0, 'gaussian_vae_lr': 0.0001, 'gaussian_vae_weight_decay': 1e-06}\n"
     ]
    }
   ],
   "source": [
    "config = load_config()\n",
    "\n",
    "for key in config:\n",
    "    locals()[key] = config[key]\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b39d724",
   "metadata": {},
   "source": [
    "## Init the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc0d709",
   "metadata": {},
   "source": [
    "## Playground  \\~( ˘▾˘~)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf5e43c",
   "metadata": {},
   "source": [
    "## AC training loop for Cartpole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1607997a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment hyperparams\n",
    "n_envs = 1\n",
    "n_updates = 30000\n",
    "n_steps_per_update = 128\n",
    "\n",
    "agent = ContinuousActorCritic(\n",
    "     n_features=3, \n",
    "     n_actions=1,\n",
    "     n_envs=n_envs,\n",
    "     gamma=0.999,\n",
    "     lam=0.95,\n",
    "     entropy_coeff=0.01,\n",
    "     critic_lr=5e-4, # it's very sensitive to higher learning rates (gets nans)\n",
    "     actor_lr=1e-4,\n",
    "    action_clip=2\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c911c88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a wrapper environment to save episode returns and episode lengths\n",
    "envs = gym.vector.make(\"Pendulum-v1\", num_envs=n_envs, max_episode_steps=200)\n",
    "envs_wrapper = gym.wrappers.RecordEpisodeStatistics(envs, deque_size=n_envs*n_updates)\n",
    "\n",
    "# Logging\n",
    "log_dir = \"logs/\"\n",
    "writer = SummaryWriter(log_dir)\n",
    "notebook.start(f\"--logdir={log_dir}\")\n",
    "\n",
    "critic_losses = []\n",
    "actor_losses = []\n",
    "entropies = []\n",
    "\n",
    "for sample_phase in tqdm(range(n_updates)):\n",
    "    \n",
    "    # we don't have to reset the envs, they just continue playing\n",
    "    # until the episode is over and then reset automatically\n",
    "\n",
    "    ep_value_preds = torch.zeros(n_steps_per_update, n_envs, device=device)\n",
    "    ep_rewards = torch.zeros(n_steps_per_update, n_envs, device=device)\n",
    "    ep_log_probs = torch.zeros(n_steps_per_update, n_envs, device=device)\n",
    "    ep_entropies = torch.zeros(n_steps_per_update, n_envs, device=device)\n",
    "    ep_masks = torch.zeros(n_steps_per_update, n_envs, device=device)\n",
    "\n",
    "    if sample_phase == 0:\n",
    "        obs, info = envs_wrapper.reset(seed=0)\n",
    "        obs = torch.Tensor(obs)\n",
    "\n",
    "    for step in range(n_steps_per_update):\n",
    "        \n",
    "        # get action and value\n",
    "        action, log_prob, actor_entropy = agent.get_action(obs)\n",
    "        value_pred = agent.critic(obs)\n",
    "\n",
    "        # env step\n",
    "        obs, reward, terminated, truncated, infos = envs_wrapper.step(to_np(action))\n",
    "        obs = torch.Tensor(obs)\n",
    "\n",
    "        ep_value_preds[step] = value_pred.squeeze()\n",
    "        ep_rewards[step] = torch.tensor(reward, device=device)\n",
    "        ep_log_probs[step] = log_prob.squeeze()\n",
    "        ep_entropies[step] = actor_entropy.squeeze()\n",
    "\n",
    "        # add a mask (for the return calculation later);\n",
    "        # for each env the mask is 1 if the episode is ongoing and 0 if it is terminated (not by truncation!)\n",
    "        dones = np.logical_or(terminated, truncated)\n",
    "        ep_masks[step] = torch.Tensor([not term for term in dones], device=device) # terminated/dones\n",
    "\n",
    "    # calculate the losses for actor and critic\n",
    "    last_value_pred = agent.critic(obs)\n",
    "    critic_loss, actor_loss = agent.get_loss(ep_rewards, ep_log_probs, ep_value_preds, last_value_pred,\n",
    "                                             ep_entropies, ep_masks)\n",
    "\n",
    "    # update the actor and critic networks\n",
    "    agent.update_parameters(critic_loss, actor_loss)\n",
    "\n",
    "    # log the losses and entropy\n",
    "    if envs_wrapper.return_queue:\n",
    "        writer.add_scalar(\"episode_return\", envs_wrapper.return_queue[-1], global_step=sample_phase)\n",
    "    writer.add_scalar(\"actor_loss\", to_np(actor_loss), global_step=sample_phase)\n",
    "    writer.add_scalar(\"critic_loss\", to_np(critic_loss), global_step=sample_phase)\n",
    "    writer.add_scalar(\"actor_entropy\", to_np(ep_entropies.mean()), global_step=sample_phase)\n",
    "    critic_losses.append(to_np(critic_loss))\n",
    "    actor_losses.append(to_np(actor_loss))\n",
    "    entropies.append(to_np(ep_entropies.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecfb3b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb0b387",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dc1abb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab060466",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf74a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Plot the results \"\"\"\n",
    "\n",
    "rolling_length = max(1, int(len(envs_wrapper.return_queue) / 20))\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(12, 5))\n",
    "fig.suptitle(f\"Training plots for {agent.__class__.__name__} with n_envs={n_envs}, n_steps_per_update={n_steps_per_update}\")\n",
    "\n",
    "# Episode return\n",
    "axs[0, 0].set_title(\"Episode Returns\")\n",
    "episode_returns_moving_average = np.convolve(np.array(envs_wrapper.return_queue).flatten(), np.ones(rolling_length), mode=\"valid\") / rolling_length\n",
    "axs[0, 0].plot(np.arange(len(episode_returns_moving_average)) / n_envs, episode_returns_moving_average)\n",
    "axs[0, 0].set_xlabel(\"Number of episodes\")\n",
    "\n",
    "# Entropy\n",
    "axs[1, 0].set_title(\"Entropy\")\n",
    "entropy_moving_average = np.convolve(np.array(entropies), np.ones(rolling_length), mode=\"valid\") / rolling_length\n",
    "axs[1, 0].plot(entropy_moving_average)\n",
    "axs[1, 0].set_xlabel(\"Number of updates\")\n",
    "\n",
    "# Critic loss\n",
    "axs[0, 1].set_title(\"Critic Loss\")\n",
    "critic_losses_moving_average = np.convolve(np.array(critic_losses).flatten(), np.ones(rolling_length), mode=\"valid\") / rolling_length\n",
    "axs[0, 1].plot(critic_losses_moving_average)\n",
    "axs[0, 1].set_xlabel(\"Number of updates\")\n",
    "\n",
    "# Actor loss\n",
    "axs[1, 1].set_title(\"Actor Loss\")\n",
    "actor_losses_moving_average = np.convolve(np.array(actor_losses).flatten(), np.ones(rolling_length), mode=\"valid\") / rolling_length\n",
    "axs[1, 1].plot(actor_losses_moving_average)\n",
    "axs[1, 1].set_xlabel(\"Number of updates\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2932f67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_length = max(1, int(len(envs_wrapper.return_queue) / 50))\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(12, 7))\n",
    "fig.suptitle(f\"Training plots for {agent.__class__.__name__} with n_envs={n_envs}, n_steps_per_update={n_steps_per_update}\")\n",
    "\n",
    "# Episode return\n",
    "axs[0, 0].set_title(\"Episode Returns\")\n",
    "episode_returns = np.array(envs_wrapper.return_queue).flatten()\n",
    "episode_returns_moving_average = np.convolve(episode_returns, np.ones(rolling_length), mode=\"valid\") / rolling_length\n",
    "episode_returns_std = np.array([np.std(episode_returns[max(0, i-rolling_length+1):i+1]) for i in range(rolling_length-1, len(episode_returns))])\n",
    "axs[0, 0].plot(np.arange(len(episode_returns_moving_average)) / n_envs, episode_returns_moving_average)\n",
    "axs[0, 0].fill_between(np.arange(len(episode_returns_moving_average)) / n_envs,\n",
    "                       episode_returns_moving_average - episode_returns_std,\n",
    "                       episode_returns_moving_average + episode_returns_std,\n",
    "                       alpha=0.3)\n",
    "axs[0, 0].set_xlabel(\"Number of episodes\")\n",
    "axs[0, 0].set_ylim(np.percentile(episode_returns, 2), np.percentile(episode_returns, 99))\n",
    "\n",
    "# Entropy\n",
    "axs[1, 0].set_title(\"Entropy\")\n",
    "entropy_moving_average = np.convolve(np.array(entropies), np.ones(rolling_length), mode=\"valid\") / rolling_length\n",
    "entropy_std = np.array([np.std(np.array(entropies)[max(0, i-rolling_length+1):i+1]) for i in range(rolling_length-1, len(entropies))])\n",
    "axs[1, 0].plot(entropy_moving_average)\n",
    "axs[1, 0].fill_between(np.arange(len(entropy_moving_average)),\n",
    "                       entropy_moving_average - entropy_std,\n",
    "                       entropy_moving_average + entropy_std,\n",
    "                       alpha=0.3)\n",
    "axs[1, 0].set_xlabel(\"Number of updates\")\n",
    "\n",
    "# Critic loss\n",
    "axs[0, 1].set_title(\"Critic Loss\")\n",
    "critic_losses = np.array(critic_losses).flatten()\n",
    "critic_losses_moving_average = np.convolve(critic_losses, np.ones(rolling_length), mode=\"valid\") / rolling_length\n",
    "critic_losses_std = np.array([np.std(critic_losses[max(0, i-rolling_length+1):i+1]) for i in range(rolling_length-1, len(critic_losses))])\n",
    "axs[0, 1].plot(critic_losses_moving_average)\n",
    "axs[0, 1].fill_between(np.arange(len(critic_losses_moving_average)),\n",
    "                       critic_losses_moving_average - critic_losses_std,\n",
    "                       critic_losses_moving_average + critic_losses_std,\n",
    "                       alpha=0.3)\n",
    "axs[0, 1].set_xlabel(\"Number of updates\")\n",
    "axs[0, 1].set_ylim(np.percentile(critic_losses, 1), np.percentile(critic_losses, 98.5))\n",
    "\n",
    "# Actor loss\n",
    "axs[1, 1].set_title(\"Actor Loss\")\n",
    "actor_losses = np.array(actor_losses).flatten()\n",
    "actor_losses_moving_average = np.convolve(actor_losses, np.ones(rolling_length), mode=\"valid\") / rolling_length\n",
    "actor_losses_std = np.array([np.std(actor_losses[max(0, i-rolling_length+1):i+1]) for i in range(rolling_length-1, len(actor_losses))])\n",
    "axs[1, 1].plot(actor_losses_moving_average)\n",
    "axs[1, 1].fill_between(np.arange(len(actor_losses_moving_average)),\n",
    "actor_losses_moving_average - actor_losses_std,\n",
    "actor_losses_moving_average + actor_losses_std,\n",
    "alpha=0.3)\n",
    "axs[1, 1].set_xlabel(\"Number of updates\")\n",
    "axs[1, 1].set_ylim(np.percentile(actor_losses, 2), np.percentile(actor_losses, 98.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e1830b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd33779",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9cad3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7deba9e",
   "metadata": {},
   "source": [
    "## Showcase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1909b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = gym.vector.make(\"Pendulum-v1\", num_envs=1, max_episode_steps=200, render_mode=\"human\")\n",
    "\n",
    "obs, info = envs.reset(seed=0)\n",
    "obs = torch.Tensor(obs)\n",
    "\n",
    "for step in range(n_steps_per_update):\n",
    "    \n",
    "    # get action and value\n",
    "    action, action_log_probs, actor_entropy = agent.get_action(obs)\n",
    "    \n",
    "    # env step\n",
    "    obs, rewards, terminated, truncated, infos = envs.step(to_np(action))\n",
    "    obs = torch.Tensor(obs)\n",
    "\n",
    "envs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a69e16b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51fd4b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c4d8ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d388651e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a75d0a7",
   "metadata": {},
   "source": [
    "## Stable Baselines 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe74ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "vec_env = make_vec_env(\"Pendulum-v1\", n_envs=1, seed=0)\n",
    "\n",
    "agent = SAC(\"MlpPolicy\", vec_env, train_freq=1, gradient_steps=-1, verbose=1, tensorboard_log=\"logs/\")\n",
    "agent.learn(total_timesteps=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df42df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "mean_reward, std_reward = evaluate_policy(agent, agent.get_env(), n_eval_episodes=3)\n",
    "mean_reward, std_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3beb1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# showcase\n",
    "showcase_env = gym.make(\"Pendulum-v1\", render_mode=\"human\")\n",
    "\n",
    "obs, info = showcase_env.reset()\n",
    "n_steps = 100\n",
    "for step in range(n_steps):\n",
    "    action, _ = agent.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, info = showcase_env.step(action)\n",
    "    showcase_env.render()\n",
    "    if done:\n",
    "        pass\n",
    "        #break\n",
    "\n",
    "showcase_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bbf747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorboard Logging\n",
    "# log_dir = \"logs/\"\n",
    "# notebook.start(f\"--logdir={log_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f18c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = DonkeyEnv(env_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "245e66b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting DonkeyGym env\n",
      "Setting default: start_delay 5.0\n",
      "Setting default: max_cte 8.0\n",
      "Setting default: frame_skip 1\n",
      "Setting default: cam_resolution (120, 160, 3)\n",
      "Setting default: log_level 20\n",
      "Setting default: host localhost\n",
      "Setting default: steer_limit 1.0\n",
      "Setting default: throttle_min 0.0\n",
      "Setting default: throttle_max 1.0\n",
      "donkey subprocess started\n",
      "Found path: /home/till/Desktop/Thesis/donkeycar_sim/DonkeySimLinux/donkey_sim.x86_64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gym_donkeycar.core.client:connecting to localhost:9091 \n",
      "/home/till/.local/lib/python3.10/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n",
      "WARNING:gym_donkeycar.envs.donkey_sim:waiting for sim to start..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading scene mini_monaco\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gym_donkeycar.envs.donkey_sim:on need car config\n",
      "INFO:gym_donkeycar.envs.donkey_sim:sending car config.\n",
      "INFO:gym_donkeycar.envs.donkey_sim:sim started!\n"
     ]
    }
   ],
   "source": [
    "# Create the environment\n",
    "sim_config = {\n",
    "    \"exe_path\" : \"/home/till/Desktop/Thesis/donkeycar_sim/DonkeySimLinux/donkey_sim.x86_64\",\n",
    "    \"port\" : 9091\n",
    "}\n",
    "env = gym.make(\n",
    "    \"GymV21Environment-v0\", \n",
    "    env_id=env_id,\n",
    "    max_episode_steps=1000,\n",
    "    make_kwargs={\n",
    "        \"conf\": sim_config\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f11f84ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Logging to logs/SAC_27\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 36.2     |\n",
      "|    ep_rew_mean     | 26.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 9        |\n",
      "|    time_elapsed    | 15       |\n",
      "|    total_timesteps | 145      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.43    |\n",
      "|    critic_loss     | 0.296    |\n",
      "|    ent_coef        | 0.987    |\n",
      "|    ent_coef_loss   | -0.0433  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 44       |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 55.8     |\n",
      "|    ep_rew_mean     | 57.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 12       |\n",
      "|    time_elapsed    | 35       |\n",
      "|    total_timesteps | 446      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -4.74    |\n",
      "|    critic_loss     | 0.0454   |\n",
      "|    ent_coef        | 0.902    |\n",
      "|    ent_coef_loss   | -0.347   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 345      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 52.2     |\n",
      "|    ep_rew_mean     | 48.6     |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 12       |\n",
      "|    time_elapsed    | 48       |\n",
      "|    total_timesteps | 626      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.96    |\n",
      "|    critic_loss     | 0.107    |\n",
      "|    ent_coef        | 0.855    |\n",
      "|    ent_coef_loss   | -0.529   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 525      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 54.2     |\n",
      "|    ep_rew_mean     | 52.6     |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 13       |\n",
      "|    time_elapsed    | 65       |\n",
      "|    total_timesteps | 868      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.41    |\n",
      "|    critic_loss     | 0.0717   |\n",
      "|    ent_coef        | 0.795    |\n",
      "|    ent_coef_loss   | -0.773   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 767      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 54.6     |\n",
      "|    ep_rew_mean     | 51.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 13       |\n",
      "|    time_elapsed    | 81       |\n",
      "|    total_timesteps | 1093     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.92    |\n",
      "|    critic_loss     | 0.0751   |\n",
      "|    ent_coef        | 0.743    |\n",
      "|    ent_coef_loss   | -0.999   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 992      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 56       |\n",
      "|    ep_rew_mean     | 53.5     |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 13       |\n",
      "|    time_elapsed    | 98       |\n",
      "|    total_timesteps | 1344     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -10.9    |\n",
      "|    critic_loss     | 0.104    |\n",
      "|    ent_coef        | 0.689    |\n",
      "|    ent_coef_loss   | -1.25    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1243     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 56.5     |\n",
      "|    ep_rew_mean     | 53.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 13       |\n",
      "|    time_elapsed    | 115      |\n",
      "|    total_timesteps | 1582     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -12      |\n",
      "|    critic_loss     | 0.111    |\n",
      "|    ent_coef        | 0.641    |\n",
      "|    ent_coef_loss   | -1.49    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1481     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 56       |\n",
      "|    ep_rew_mean     | 52.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 13       |\n",
      "|    time_elapsed    | 130      |\n",
      "|    total_timesteps | 1793     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -13.8    |\n",
      "|    critic_loss     | 0.273    |\n",
      "|    ent_coef        | 0.602    |\n",
      "|    ent_coef_loss   | -1.71    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1692     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 57       |\n",
      "|    ep_rew_mean     | 53.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 13       |\n",
      "|    time_elapsed    | 147      |\n",
      "|    total_timesteps | 2052     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -15      |\n",
      "|    critic_loss     | 0.287    |\n",
      "|    ent_coef        | 0.557    |\n",
      "|    ent_coef_loss   | -1.97    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1951     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 59.2     |\n",
      "|    ep_rew_mean     | 56.5     |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 14       |\n",
      "|    time_elapsed    | 168      |\n",
      "|    total_timesteps | 2369     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -18.8    |\n",
      "|    critic_loss     | 0.49     |\n",
      "|    ent_coef        | 0.507    |\n",
      "|    ent_coef_loss   | -2.29    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2268     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 60.2     |\n",
      "|    ep_rew_mean     | 59       |\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 14       |\n",
      "|    time_elapsed    | 186      |\n",
      "|    total_timesteps | 2647     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -19      |\n",
      "|    critic_loss     | 0.43     |\n",
      "|    ent_coef        | 0.466    |\n",
      "|    ent_coef_loss   | -2.52    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2546     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 61.2     |\n",
      "|    ep_rew_mean     | 61.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 48       |\n",
      "|    fps             | 14       |\n",
      "|    time_elapsed    | 206      |\n",
      "|    total_timesteps | 2938     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -20.8    |\n",
      "|    critic_loss     | 0.83     |\n",
      "|    ent_coef        | 0.428    |\n",
      "|    ent_coef_loss   | -2.76    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2837     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 16\u001b[0m\n\u001b[1;32m      2\u001b[0m agent \u001b[38;5;241m=\u001b[39m SAC(\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCnnPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m      4\u001b[0m     env,\n\u001b[1;32m      5\u001b[0m     buffer_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20_000\u001b[39m,\n\u001b[1;32m      6\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, \n\u001b[1;32m      7\u001b[0m     tensorboard_log\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogs/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# eval_callback = EvalCallback(\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#     env, \u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#     best_model_save_path='weights/', \u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#     log_path='logs/', \u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#     eval_freq=500,\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#     n_eval_episodes=1)\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30_000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/sac/sac.py:307\u001b[0m, in \u001b[0;36mSAC.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfSAC,\n\u001b[1;32m    300\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    306\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfSAC:\n\u001b[0;32m--> 307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/off_policy_algorithm.py:312\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    309\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_start(\u001b[38;5;28mlocals\u001b[39m(), \u001b[38;5;28mglobals\u001b[39m())\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 312\u001b[0m     rollout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43maction_noise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_noise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_starts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_starts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rollout\u001b[38;5;241m.\u001b[39mcontinue_training \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/off_policy_algorithm.py:544\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[0m\n\u001b[1;32m    541\u001b[0m actions, buffer_actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample_action(learning_starts, action_noise, env\u001b[38;5;241m.\u001b[39mnum_envs)\n\u001b[1;32m    543\u001b[0m \u001b[38;5;66;03m# Rescale and perform action\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[1;32m    547\u001b[0m num_collected_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:171\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/vec_env/vec_transpose.py:95\u001b[0m, in \u001b[0;36mVecTransposeImage.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m---> 95\u001b[0m     observations, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvenv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;66;03m# Transpose the terminal observations\u001b[39;00m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, done \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dones):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:57\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m---> 57\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneeds_reset:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to step environment that needs reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 94\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(reward))\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/wrappers/time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/wrappers/order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/wrappers/env_checker.py:49\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/shimmy/openai_gym_compatibility.py:255\u001b[0m, in \u001b[0;36mGymV21CompatibilityV0.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: ActType) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[Any, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m]:\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;124;03m\"\"\"Steps through the environment.\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \n\u001b[1;32m    249\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;124;03m        (observation, reward, terminated, truncated, info)\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 255\u001b[0m     obs, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgym_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gym/wrappers/order_enforcing.py:11\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 11\u001b[0m     observation, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m observation, reward, done, info\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gym_donkeycar/envs/donkey_env.py:136\u001b[0m, in \u001b[0;36mDonkeyEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframe_skip):\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mviewer\u001b[38;5;241m.\u001b[39mtake_action(action)\n\u001b[0;32m--> 136\u001b[0m     observation, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mviewer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobserve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m observation, reward, done, info\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gym_donkeycar/envs/donkey_sim.py:108\u001b[0m, in \u001b[0;36mDonkeyUnitySimContoller.observe\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobserve\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mbool\u001b[39m, Dict[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m--> 108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobserve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gym_donkeycar/envs/donkey_sim.py:444\u001b[0m, in \u001b[0;36mDonkeyUnitySimHandler.observe\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobserve\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mbool\u001b[39m, Dict[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_received \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_received:\n\u001b[0;32m--> 444\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_received \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_received\n\u001b[1;32m    447\u001b[0m     observation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_array\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent = SAC(\n",
    "    \"CnnPolicy\", \n",
    "    env,\n",
    "    buffer_size=20_000,\n",
    "    verbose=1, \n",
    "    tensorboard_log=\"logs/\")\n",
    "\n",
    "# eval_callback = EvalCallback(\n",
    "#     env, \n",
    "#     best_model_save_path='weights/', \n",
    "#     log_path='logs/', \n",
    "#     eval_freq=500,\n",
    "#     n_eval_episodes=1)\n",
    "\n",
    "agent.learn(total_timesteps=30_000) # callback=eval_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee148888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training results from the log files\n",
    "results = load_results('logs/')\n",
    "\n",
    "# Extract the training curve data\n",
    "x, y = ts2xy(results, 'timesteps')\n",
    "\n",
    "# Plot the training curve\n",
    "plt.plot(x, y)\n",
    "plt.xlabel('Timesteps')\n",
    "plt.ylabel('Rewards')\n",
    "plt.title('Training Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8530860e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# envs = gym.vector.make(\"Pendulum-v1\", num_envs=1, max_episode_steps=200, render_mode=\"human\")\n",
    "# \n",
    "# obs, info = envs.reset(seed=0)\n",
    "# obs = torch.Tensor(obs)\n",
    "# \n",
    "# for step in range(n_steps_per_update):\n",
    "#     \n",
    "#     # get action and value\n",
    "#     action, state_value, action_log_probs, actor_entropy = agent.select_action(obs)\n",
    "#     continuous_action = []\n",
    "#     for idx in actions:\n",
    "#         continuous_action.append([idx_to_action[idx.item()]])\n",
    "#     #print(continuous_action)\n",
    "#     # env step\n",
    "#     obs, rewards, terminated, truncated, infos = envs.step(continuous_action)\n",
    "#     obs = torch.Tensor(obs)\n",
    "# \n",
    "# envs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dafadb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47325f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt.space import Real, Integer\n",
    "from skopt.utils import use_named_args\n",
    "from skopt import gp_minimize\n",
    "from skopt.plots import plot_convergence, plot_objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011a4069",
   "metadata": {},
   "outputs": [],
   "source": [
    "space  = [\n",
    "    Real(0, 1, name=\"gamma\"),\n",
    "    Real(0, 1, name=\"lam\"),\n",
    "    Real(10**-10, 10**4, \"log-uniform\", name=\"ent_coef\"),\n",
    "    Real(10**-5, 10**0, \"log-uniform\", name=\"actor_lr\"),\n",
    "    Real(10**-5, 10**0, \"log-uniform\", name=\"critic_lr\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9f7d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # environment hyperparams\n",
    "# n_envs = 1\n",
    "# n_updates = 1000\n",
    "# n_steps_per_update = 128\n",
    "# \n",
    "# # agent hyperparams\n",
    "# gamma = 0.999\n",
    "# lam = 0.95  # hyperparameter for GAE\n",
    "# ent_coef = 0.01  # coefficient for the entropy bonus (to encourage exploration)\n",
    "# actor_lr = 0.001\n",
    "# critic_lr = 0.005\n",
    "# \n",
    "# agent = A2C(3, 9, device, critic_lr, actor_lr, n_envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be17626a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    \"\"\"  The objective we want to MINIMIZE \"\"\"\n",
    "    \n",
    "    # for a neural net\n",
    "    # model.set_params(**params)\n",
    "    # return -np.mean(cross_val_score(reg, X, y, cv=5, n_jobs=-1, scoring=\"neg_mean_absolute_error\"))\n",
    "    \n",
    "    print(params)\n",
    "    # result = 0\n",
    "    # for x in params.values():\n",
    "    #     result -= x\n",
    "    # return result\n",
    "    \n",
    "    # environment hyperparams\n",
    "    n_envs = 1\n",
    "    n_updates = 1000\n",
    "    n_steps_per_update = 128\n",
    "\n",
    "    # agent hyperparams\n",
    "    gamma = params[\"gamma\"]\n",
    "    lam = params[\"lam\"]\n",
    "    ent_coef = params[\"ent_coef\"]\n",
    "    actor_lr = params[\"actor_lr\"]\n",
    "    critic_lr = params[\"critic_lr\"]\n",
    "\n",
    "    agent = A2C(3, 9, device, critic_lr, actor_lr, n_envs, gamma, lam, ent_coef)\n",
    "\n",
    "    return_queue, critic_losses, actor_losses, entropies = train(\n",
    "        n_envs, n_updates, n_steps_per_update) # we want to maximize this\n",
    "    \n",
    "    result = - sum(return_queue) # we want to minimize this\n",
    "    # print(\"result:\", result, \"(we want to minimize this)\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8525c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import gp_minimize\n",
    "gp_result = gp_minimize(\n",
    "    objective, \n",
    "    space, \n",
    "    n_calls=20, \n",
    "    random_state=0)\n",
    "\n",
    "print(\"Best score:\", gp_result.fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2173e7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_convergence(gp_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621c4800",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_objective(gp_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2461144",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" train an agent with the best params \"\"\"\n",
    "\n",
    "print(\"best parameters:\", gp_result.x)\n",
    "\n",
    "gamma, lam, ent_coef, actor_lr, critic_lr = gp_result.x\n",
    "agent = A2C(3, 9, device, critic_lr, actor_lr, n_envs, gamma, lam, ent_coef)\n",
    "\n",
    "# set longer training time\n",
    "n_envs = 1\n",
    "n_updates = 3000\n",
    "n_steps_per_update = 128\n",
    "\n",
    "return_queue, critic_losses, actor_losses, entropies = train(n_envs, n_updates, n_steps_per_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c132db1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fcdb15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7638258",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2410921b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3191a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353ace5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a571d54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922ced73",
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = gym.vector.make(\"Pendulum-v1\", num_envs=1, max_episode_steps=200, render_mode=\"human\")\n",
    "\n",
    "obs, info = envs.reset(seed=0)\n",
    "obs = torch.Tensor(obs)\n",
    "\n",
    "for step in range(n_steps_per_update):\n",
    "    \n",
    "    # get action and value\n",
    "    action, state_value, action_log_probs, actor_entropy = agent.select_action(obs)\n",
    "    continuous_action = []\n",
    "    for idx in actions:\n",
    "        continuous_action.append([idx_to_action[idx.item()]])\n",
    "    #print(continuous_action)\n",
    "    # env step\n",
    "    obs, rewards, terminated, truncated, infos = envs.step(continuous_action)\n",
    "    obs = torch.Tensor(obs)\n",
    "\n",
    "envs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d493437",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
