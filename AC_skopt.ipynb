{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b1c5dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/till/.local/lib/python3.10/site-packages/gymnasium/envs/registration.py:498: UserWarning: \u001b[33mWARN: Overriding environment GymV26Environment-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import gym_donkeycar\n",
    "\n",
    "import os\n",
    "from ruamel.yaml import YAML\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import HBox, VBox\n",
    "from IPython.display import display\n",
    "from tqdm import tqdm\n",
    "\n",
    "from PIL import Image\n",
    "import imageio\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import distributions as dist\n",
    "from torch.distributions import Normal, Categorical\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from tensorboard import notebook\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# suppress warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ[\"IMAGEIO_IGNORE_WARNINGS\"] = \"True\"\n",
    "\n",
    "from networks.utils import to_np, load_config, save_image_and_reconstruction\n",
    "\n",
    "# custom classes and functions\n",
    "from networks.blocks import ConvBlock, CategoricalStraightThrough\n",
    "from networks.rssm import RSSM\n",
    "from networks.mlp import MLP\n",
    "from networks.categorical_vae import CategoricalVAE\n",
    "from networks.actor_critic import ContinuousActorCritic\n",
    "from preprocessing import grayscale_transform as transform\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1af578e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b39d724",
   "metadata": {},
   "source": [
    "## Init the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc0d709",
   "metadata": {},
   "source": [
    "## Playground  \\~( ˘▾˘~)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf5e43c",
   "metadata": {},
   "source": [
    "## AC training loop for Cartpole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1607997a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment hyperparams\n",
    "n_envs = 1\n",
    "n_updates = 30000\n",
    "n_steps_per_update = 128\n",
    "\n",
    "agent = ContinuousActorCritic(\n",
    "     n_features=3, \n",
    "     n_actions=1,\n",
    "     n_envs=n_envs,\n",
    "     gamma=0.999,\n",
    "     lam=0.95,\n",
    "     entropy_coeff=0.01,\n",
    "     critic_lr=5e-4, # it's very sensitive to higher learning rates (gets nans)\n",
    "     actor_lr=1e-4,\n",
    "    action_clip=2\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c911c88b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-54b562b621353a12\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-54b562b621353a12\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                 | 0/30000 [00:00<?, ?it/s]/home/till/.local/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "  6%|█████                                                                               | 1818/30000 [07:18<1:48:18,  4.34it/s]"
     ]
    }
   ],
   "source": [
    "# create a wrapper environment to save episode returns and episode lengths\n",
    "envs = gym.vector.make(\"Pendulum-v1\", num_envs=n_envs, max_episode_steps=200)\n",
    "envs_wrapper = gym.wrappers.RecordEpisodeStatistics(envs, deque_size=n_envs*n_updates)\n",
    "\n",
    "# Logging\n",
    "log_dir = \"logs/\"\n",
    "writer = SummaryWriter(log_dir)\n",
    "notebook.start(f\"--logdir={log_dir}\")\n",
    "\n",
    "critic_losses = []\n",
    "actor_losses = []\n",
    "entropies = []\n",
    "\n",
    "for sample_phase in tqdm(range(n_updates)):\n",
    "    \n",
    "    # we don't have to reset the envs, they just continue playing\n",
    "    # until the episode is over and then reset automatically\n",
    "\n",
    "    ep_value_preds = torch.zeros(n_steps_per_update, n_envs, device=device)\n",
    "    ep_rewards = torch.zeros(n_steps_per_update, n_envs, device=device)\n",
    "    ep_log_probs = torch.zeros(n_steps_per_update, n_envs, device=device)\n",
    "    ep_entropies = torch.zeros(n_steps_per_update, n_envs, device=device)\n",
    "    ep_masks = torch.zeros(n_steps_per_update, n_envs, device=device)\n",
    "\n",
    "    if sample_phase == 0:\n",
    "        obs, info = envs_wrapper.reset(seed=0)\n",
    "        obs = torch.Tensor(obs)\n",
    "\n",
    "    for step in range(n_steps_per_update):\n",
    "        \n",
    "        # get action and value\n",
    "        action, log_prob, actor_entropy = agent.get_action(obs)\n",
    "        value_pred = agent.critic(obs)\n",
    "\n",
    "        # env step\n",
    "        obs, reward, terminated, truncated, infos = envs_wrapper.step(to_np(action))\n",
    "        obs = torch.Tensor(obs)\n",
    "\n",
    "        ep_value_preds[step] = value_pred.squeeze()\n",
    "        ep_rewards[step] = torch.tensor(reward, device=device)\n",
    "        ep_log_probs[step] = log_prob.squeeze()\n",
    "        ep_entropies[step] = actor_entropy.squeeze()\n",
    "\n",
    "        # add a mask (for the return calculation later);\n",
    "        # for each env the mask is 1 if the episode is ongoing and 0 if it is terminated (not by truncation!)\n",
    "        # dones = np.logical_or(terminated, truncated)\n",
    "        ep_masks[step] = torch.Tensor([not term for term in terminated], device=device) # terminated/dones\n",
    "\n",
    "    # calculate the losses for actor and critic\n",
    "    last_value_pred = agent.critic(obs)\n",
    "    critic_loss, actor_loss = agent.get_loss(ep_rewards, ep_log_probs, ep_value_preds, last_value_pred,\n",
    "                                             ep_entropies, ep_masks)\n",
    "\n",
    "    # update the actor and critic networks\n",
    "    agent.update_parameters(critic_loss, actor_loss)\n",
    "\n",
    "    # log the losses and entropy\n",
    "    if envs_wrapper.return_queue:\n",
    "        writer.add_scalar(\"episode_return\", envs_wrapper.return_queue[-1], global_step=sample_phase)\n",
    "    writer.add_scalar(\"actor_loss\", to_np(actor_loss), global_step=sample_phase)\n",
    "    writer.add_scalar(\"critic_loss\", to_np(critic_loss), global_step=sample_phase)\n",
    "    writer.add_scalar(\"actor_entropy\", to_np(ep_entropies.mean()), global_step=sample_phase)\n",
    "    critic_losses.append(to_np(critic_loss))\n",
    "    actor_losses.append(to_np(actor_loss))\n",
    "    entropies.append(to_np(ep_entropies.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de412189",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531cd6a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c11095d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368ad88d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf74a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Plot the results \"\"\"\n",
    "\n",
    "rolling_length = max(1, int(len(envs_wrapper.return_queue) / 20))\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(12, 5))\n",
    "fig.suptitle(f\"Training plots for {agent.__class__.__name__} with n_envs={n_envs}, n_steps_per_update={n_steps_per_update}\")\n",
    "\n",
    "# Episode return\n",
    "axs[0, 0].set_title(\"Episode Returns\")\n",
    "episode_returns_moving_average = np.convolve(np.array(envs_wrapper.return_queue).flatten(), np.ones(rolling_length), mode=\"valid\") / rolling_length\n",
    "axs[0, 0].plot(np.arange(len(episode_returns_moving_average)) / n_envs, episode_returns_moving_average)\n",
    "axs[0, 0].set_xlabel(\"Number of episodes\")\n",
    "\n",
    "# Entropy\n",
    "axs[1, 0].set_title(\"Entropy\")\n",
    "entropy_moving_average = np.convolve(np.array(entropies), np.ones(rolling_length), mode=\"valid\") / rolling_length\n",
    "axs[1, 0].plot(entropy_moving_average)\n",
    "axs[1, 0].set_xlabel(\"Number of updates\")\n",
    "\n",
    "# Critic loss\n",
    "axs[0, 1].set_title(\"Critic Loss\")\n",
    "critic_losses_moving_average = np.convolve(np.array(critic_losses).flatten(), np.ones(rolling_length), mode=\"valid\") / rolling_length\n",
    "axs[0, 1].plot(critic_losses_moving_average)\n",
    "axs[0, 1].set_xlabel(\"Number of updates\")\n",
    "\n",
    "# Actor loss\n",
    "axs[1, 1].set_title(\"Actor Loss\")\n",
    "actor_losses_moving_average = np.convolve(np.array(actor_losses).flatten(), np.ones(rolling_length), mode=\"valid\") / rolling_length\n",
    "axs[1, 1].plot(actor_losses_moving_average)\n",
    "axs[1, 1].set_xlabel(\"Number of updates\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7deba9e",
   "metadata": {},
   "source": [
    "## Showcase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1909b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = gym.vector.make(\"Pendulum-v1\", num_envs=1, max_episode_steps=200, render_mode=\"human\")\n",
    "\n",
    "obs, info = envs.reset(seed=0)\n",
    "obs = torch.Tensor(obs)\n",
    "\n",
    "for step in range(n_steps_per_update):\n",
    "    \n",
    "    # get action and value\n",
    "    action, action_log_probs, actor_entropy = agent.get_action(obs)\n",
    "    \n",
    "    # env step\n",
    "    obs, rewards, terminated, truncated, infos = envs.step(to_np(action))\n",
    "    obs = torch.Tensor(obs)\n",
    "\n",
    "envs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a69e16b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51fd4b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c4d8ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11caa31b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d388651e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c24fc9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122ef36d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73168957",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1203751",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaf34be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c602f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8112d371",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3beb1b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8101be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c33dcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46102bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8530860e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# envs = gym.vector.make(\"Pendulum-v1\", num_envs=1, max_episode_steps=200, render_mode=\"human\")\n",
    "# \n",
    "# obs, info = envs.reset(seed=0)\n",
    "# obs = torch.Tensor(obs)\n",
    "# \n",
    "# for step in range(n_steps_per_update):\n",
    "#     \n",
    "#     # get action and value\n",
    "#     action, state_value, action_log_probs, actor_entropy = agent.select_action(obs)\n",
    "#     continuous_action = []\n",
    "#     for idx in actions:\n",
    "#         continuous_action.append([idx_to_action[idx.item()]])\n",
    "#     #print(continuous_action)\n",
    "#     # env step\n",
    "#     obs, rewards, terminated, truncated, infos = envs.step(continuous_action)\n",
    "#     obs = torch.Tensor(obs)\n",
    "# \n",
    "# envs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dafadb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47325f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt.space import Real, Integer\n",
    "from skopt.utils import use_named_args\n",
    "from skopt import gp_minimize\n",
    "from skopt.plots import plot_convergence, plot_objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011a4069",
   "metadata": {},
   "outputs": [],
   "source": [
    "space  = [\n",
    "    Real(0, 1, name=\"gamma\"),\n",
    "    Real(0, 1, name=\"lam\"),\n",
    "    Real(10**-10, 10**4, \"log-uniform\", name=\"ent_coef\"),\n",
    "    Real(10**-5, 10**0, \"log-uniform\", name=\"actor_lr\"),\n",
    "    Real(10**-5, 10**0, \"log-uniform\", name=\"critic_lr\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9f7d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # environment hyperparams\n",
    "# n_envs = 1\n",
    "# n_updates = 1000\n",
    "# n_steps_per_update = 128\n",
    "# \n",
    "# # agent hyperparams\n",
    "# gamma = 0.999\n",
    "# lam = 0.95  # hyperparameter for GAE\n",
    "# ent_coef = 0.01  # coefficient for the entropy bonus (to encourage exploration)\n",
    "# actor_lr = 0.001\n",
    "# critic_lr = 0.005\n",
    "# \n",
    "# agent = A2C(3, 9, device, critic_lr, actor_lr, n_envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be17626a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    \"\"\"  The objective we want to MINIMIZE \"\"\"\n",
    "    \n",
    "    # for a neural net\n",
    "    # model.set_params(**params)\n",
    "    # return -np.mean(cross_val_score(reg, X, y, cv=5, n_jobs=-1, scoring=\"neg_mean_absolute_error\"))\n",
    "    \n",
    "    print(params)\n",
    "    # result = 0\n",
    "    # for x in params.values():\n",
    "    #     result -= x\n",
    "    # return result\n",
    "    \n",
    "    # environment hyperparams\n",
    "    n_envs = 1\n",
    "    n_updates = 1000\n",
    "    n_steps_per_update = 128\n",
    "\n",
    "    # agent hyperparams\n",
    "    gamma = params[\"gamma\"]\n",
    "    lam = params[\"lam\"]\n",
    "    ent_coef = params[\"ent_coef\"]\n",
    "    actor_lr = params[\"actor_lr\"]\n",
    "    critic_lr = params[\"critic_lr\"]\n",
    "\n",
    "    agent = A2C(3, 9, device, critic_lr, actor_lr, n_envs, gamma, lam, ent_coef)\n",
    "\n",
    "    return_queue, critic_losses, actor_losses, entropies = train(\n",
    "        n_envs, n_updates, n_steps_per_update) # we want to maximize this\n",
    "    \n",
    "    result = - sum(return_queue) # we want to minimize this\n",
    "    # print(\"result:\", result, \"(we want to minimize this)\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8525c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import gp_minimize\n",
    "gp_result = gp_minimize(\n",
    "    objective, \n",
    "    space, \n",
    "    n_calls=20, \n",
    "    random_state=0)\n",
    "\n",
    "print(\"Best score:\", gp_result.fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2173e7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_convergence(gp_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621c4800",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_objective(gp_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2461144",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" train an agent with the best params \"\"\"\n",
    "\n",
    "print(\"best parameters:\", gp_result.x)\n",
    "\n",
    "gamma, lam, ent_coef, actor_lr, critic_lr = gp_result.x\n",
    "agent = A2C(3, 9, device, critic_lr, actor_lr, n_envs, gamma, lam, ent_coef)\n",
    "\n",
    "# set longer training time\n",
    "n_envs = 1\n",
    "n_updates = 3000\n",
    "n_steps_per_update = 128\n",
    "\n",
    "return_queue, critic_losses, actor_losses, entropies = train(n_envs, n_updates, n_steps_per_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aefd9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Plot the results \"\"\"\n",
    "\n",
    "rolling_length = max(1, int(len(return_queue) / 20))\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(12, 5))\n",
    "fig.suptitle(f\"Training plots for {agent.__class__.__name__} with n_envs={n_envs}, n_steps_per_update={n_steps_per_update}\")\n",
    "\n",
    "# Episode return\n",
    "axs[0, 0].set_title(\"Episode Returns\")\n",
    "episode_returns_moving_average = np.convolve(np.array(return_queue).flatten(), np.ones(rolling_length), mode=\"valid\") / rolling_length\n",
    "axs[0, 0].plot(np.arange(len(episode_returns_moving_average)) / n_envs, episode_returns_moving_average)\n",
    "axs[0, 0].set_xlabel(\"Number of episodes\")\n",
    "\n",
    "# Entropy\n",
    "axs[1, 0].set_title(\"Entropy\")\n",
    "entropy_moving_average = np.convolve(np.array(entropies), np.ones(rolling_length), mode=\"valid\") / rolling_length\n",
    "axs[1, 0].plot(entropy_moving_average)\n",
    "axs[1, 0].set_xlabel(\"Number of updates\")\n",
    "\n",
    "# Critic loss\n",
    "axs[0, 1].set_title(\"Critic Loss\")\n",
    "critic_losses_moving_average = np.convolve(np.array(critic_losses).flatten(), np.ones(rolling_length), mode=\"valid\") / rolling_length\n",
    "axs[0, 1].plot(critic_losses_moving_average)\n",
    "axs[0, 1].set_xlabel(\"Number of updates\")\n",
    "\n",
    "# Actor loss\n",
    "axs[1, 1].set_title(\"Actor Loss\")\n",
    "actor_losses_moving_average = np.convolve(np.array(actor_losses).flatten(), np.ones(rolling_length), mode=\"valid\") / rolling_length\n",
    "axs[1, 1].plot(actor_losses_moving_average)\n",
    "axs[1, 1].set_xlabel(\"Number of updates\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb117dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a571d54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922ced73",
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = gym.vector.make(\"Pendulum-v1\", num_envs=1, max_episode_steps=200, render_mode=\"human\")\n",
    "\n",
    "obs, info = envs.reset(seed=0)\n",
    "obs = torch.Tensor(obs)\n",
    "\n",
    "for step in range(n_steps_per_update):\n",
    "    \n",
    "    # get action and value\n",
    "    action, state_value, action_log_probs, actor_entropy = agent.select_action(obs)\n",
    "    continuous_action = []\n",
    "    for idx in actions:\n",
    "        continuous_action.append([idx_to_action[idx.item()]])\n",
    "    #print(continuous_action)\n",
    "    # env step\n",
    "    obs, rewards, terminated, truncated, infos = envs.step(continuous_action)\n",
    "    obs = torch.Tensor(obs)\n",
    "\n",
    "envs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d493437",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
