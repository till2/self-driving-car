{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3468cdc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello there!\n"
     ]
    }
   ],
   "source": [
    "print(\"hello there!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "838126cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install vit-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f01d688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# other ViTs that could be useful:\n",
    "# from https://github.com/lucidrains/vit-pytorch#vision-transformer---pytorch\n",
    "#\n",
    "# Masked Autoencoder\n",
    "# Vision Transformer for Small Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8d17aa",
   "metadata": {},
   "source": [
    "Parameters\n",
    "\n",
    "    image_size: int.\n",
    "    Image size. If you have rectangular images, make sure your image size is the maximum of the width and height\n",
    "    patch_size: int.\n",
    "    Number of patches. image_size must be divisible by patch_size.\n",
    "    The number of patches is:  n = (image_size // patch_size) ** 2 and n must be greater than 16.\n",
    "    num_classes: int.\n",
    "    Number of classes to classify.\n",
    "    dim: int.\n",
    "    Last dimension of output tensor after linear transformation nn.Linear(..., dim).\n",
    "    depth: int.\n",
    "    Number of Transformer blocks.\n",
    "    heads: int.\n",
    "    Number of heads in Multi-head Attention layer.\n",
    "    mlp_dim: int.\n",
    "    Dimension of the MLP (FeedForward) layer.\n",
    "    channels: int, default 3.\n",
    "    Number of image's channels.\n",
    "    dropout: float between [0, 1], default 0..\n",
    "    Dropout rate.\n",
    "    emb_dropout: float between [0, 1], default 0.\n",
    "    Embedding dropout rate.\n",
    "    pool: string, either cls token pooling or mean pooling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74a7657a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from vit_pytorch import ViT\n",
    "\n",
    "v = ViT(\n",
    "    image_size = 256,\n",
    "    patch_size = 32, # img -> 8 by 8 patches with size (32x32)\n",
    "    num_classes = 1000,\n",
    "    dim = 1024,\n",
    "    depth = 6,\n",
    "    heads = 16,\n",
    "    mlp_dim = 2048,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1\n",
    ")\n",
    "\n",
    "img = torch.randn(1, 3, 256, 256)\n",
    "\n",
    "preds = v(img) # (1, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc7fad8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "256/32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c349301b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
