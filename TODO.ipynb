{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1eb76a19",
   "metadata": {},
   "source": [
    "# TODO List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddeecd89",
   "metadata": {},
   "source": [
    "## [ x ] Integrate RNN into training loop (with dummy mlps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d36370",
   "metadata": {},
   "source": [
    "```py\n",
    "\n",
    "[ x ] - given: h, obs (obs is given only in training mode)\n",
    "\n",
    "# predict z from h\n",
    "[ x ] z_pred = dynamics_mlp(h)\n",
    "\n",
    "# get true z from encoder\n",
    "[ x ] z_sample = vae.encode(obs) # returns one-hot on softmax sample\n",
    "\n",
    "[ x ] state = concat(h,z)\n",
    "\n",
    "# apply actor and critic nets on state\n",
    "[ x ] v = val_net(state)\n",
    "[ x ] a = policy_net(state) # a is a 2x1 vector\n",
    "\n",
    "# predict other stuff\n",
    "[ x ] r = reward_mlp(state)\n",
    "[ x ] c = continue_mlp(state) # binary classification\n",
    "\n",
    "# combine everything into belief state\n",
    "[ x ] rnn_input = concat(a, state)\n",
    "\n",
    "\n",
    "# apply rnn\n",
    "[ x ] _, h_new = rnn(rnn_input, h)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f90dd3",
   "metadata": {},
   "source": [
    "## [ x ] Create MLPs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbd7806",
   "metadata": {},
   "source": [
    "```py\n",
    "[ x ] dynamics_mlp\n",
    "[ x ] reward_mlp\n",
    "[ x ] continue_mlp\n",
    "\n",
    "[ x ] value_net\n",
    "[ x ] policy_net\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9ec6dd",
   "metadata": {},
   "source": [
    "## [ x ] Combine Networks into RSSM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9894de9a",
   "metadata": {},
   "source": [
    "```py\n",
    "\n",
    "[ x ] vae\n",
    "[ x ] rnn\n",
    "\n",
    "[ x ] dynamics_mlp\n",
    "[ x ] reward_mlp\n",
    "[ x ] continue_mlp\n",
    "\n",
    "[ x ] forward\n",
    "[  ] info\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5543fb",
   "metadata": {},
   "source": [
    "```py\n",
    "\n",
    "[ x ] edit policy net and value net inputs to z\n",
    "[ x ] step\n",
    "[ x ] add the following to the training loop (was in step before):\n",
    "\n",
    "    [ x ] # predict z and generate the true stochastic latent variable z with the encoder\n",
    "    [ x ] z_pred = rssm.dynamics_mlp(h).view(-1, Z) # B,1024 (flattened)\n",
    "    [ x ] z = self.vae.encode(ABCD).view(-1, Z)\n",
    "    [ x ] \n",
    "    [ x ] # apply external actor and critic nets on state\n",
    "    [ x ] action = policy_net(z) # a is a 2x1 vector\n",
    "    [ x ] v = value_net(z)\n",
    "\n",
    "\n",
    "[ x ] actor net with z instead of state\n",
    "[ x ] policy net with z instead of state\n",
    "\n",
    "[x  ] sequential categorical VAE encode that takes h,x\n",
    "[x  ] sequential categorical VAE decode that takes h,z\n",
    "\n",
    "[ x ] decoder with h,z\n",
    "\n",
    "[ sp√§ter  ] hidden state batches\n",
    "\n",
    "\n",
    "[ . ] losses\n",
    "\n",
    "[  ] sample reward\n",
    "[  ] continue flag\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9662d490",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
